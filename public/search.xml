<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2023年不太顺利地过渡</title>
      <link href="/2023/12/31/live/2023-nian-bu-tai-shun-li-di-guo-du/"/>
      <url>/2023/12/31/live/2023-nian-bu-tai-shun-li-di-guo-du/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在看完一位我十分欣赏的技术大佬DIYGOD的年终总结之后，十分羡慕她如诗一般的2023，得益于她优秀的文笔，一个个碎片化的经历以图文的形式进行记录和整理，逝去的时间仿佛又生动而具体地进行了重现。受此激励，我也要完完整整地写一篇2023的年终总结!!!</p><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><h3 id="2023年1月13日"><a href="#2023年1月13日" class="headerlink" title="2023年1月13日"></a>2023年1月13日</h3><p>来网易的第一个年会，就抽到switch-新人光环666。不过一整年switch都持续吃灰，尝试甩在闲鱼，最高是有1200想收，后来想了想，还是持续吃灰吧，万一之后任天堂出了有意思且想玩的游戏呢</p><p><img src="/../../img/2023/swich1.jpg" alt="swich1.jpg"><br><img src="/../../img/2023/swich2.jpg" alt="swich2.jpg"><br><img src="/../../img/2023/swich3.jpg" alt="swich3.jpg"></p><h3 id="2023年1月22日"><a href="#2023年1月22日" class="headerlink" title="2023年1月22日"></a>2023年1月22日</h3><p>年前横跨1600公里自驾回广西，期间经过了长沙，喝了茶颜悦色，是我喝的最好喝的奶茶<br><img src="/../../img/2023/tea.jpg" alt="tea.jpg"><br>烤蜈蚣，感觉吃了就要嘎了<br><img src="/../../img/2023/food1.jpg" alt="food1.jpg"><br>长沙臭豆腐～感觉和其他地方的没啥区别呢？hhh<br><img src="/../../img/2023/food2.jpg" alt="food2.jpg"><br>便宜好喝的果汁～<br><img src="/../../img/2023/food3.jpg" alt="food3.jpg"></p><h3 id="2023年2月23日"><a href="#2023年2月23日" class="headerlink" title="2023年2月23日"></a>2023年2月23日</h3><p>时隔好些年终于再见到外婆了，脸上还爬满了皱纹，还在外婆家烤起了串～</p><p><img src="/../../img/2023/grandma1.jpg" alt="grandma1.jpg"><br><img src="/../../img/2023/grandma2.jpg" alt="grandma2.jpg"><br><img src="/../../img/2023/food4.jpg" alt="food4.jpg"></p><h3 id="2023年2月26日"><a href="#2023年2月26日" class="headerlink" title="2023年2月26日"></a>2023年2月26日</h3><p>自驾回杭州，又是经过湖南，不过这次我们爬了一趟家界，感觉风景一般般，但是山路十八弯真的够惊险的。风景很好看，以后不来了</p><p><img src="/../../img/2023/%E5%BC%A0%E5%AE%B6%E7%95%8C1.jpg" alt="张家界1.jpg"><br><img src="/../../img/2023/%E5%BC%A0%E5%AE%B6%E7%95%8C2.jpg" alt="张家界2.jpg"></p><h3 id="2023年3月1日"><a href="#2023年3月1日" class="headerlink" title="2023年3月1日"></a>2023年3月1日</h3><p>温州楠溪江音乐节，在后排的树荫下躺着充气沙发，非常惬意的听了一整天的音乐。这次来了房东的猫，夏日入侵企划，棱镜，。。这次音乐节承包了~年份的live。最后的压轴的两位，毛不易和许嵩，爷的青春又回来了</p><h3 id="2023年3月1日-1"><a href="#2023年3月1日-1" class="headerlink" title="2023年3月1日"></a>2023年3月1日</h3><p>和友人A非常惬意的早晨只知道当时的阳光很明媚当时的风很温暖</p><h3 id="2023年5月6日"><a href="#2023年5月6日" class="headerlink" title="2023年5月6日"></a>2023年5月6日</h3><p>jay在多年前七月上等爆火之后隐匿了多年，如今她带着新作品回来了，这也是第一次见到本人</p><h2 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h2><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><h2 id="2023-to-2024"><a href="#2023-to-2024" class="headerlink" title="2023 to 2024"></a>2023 to 2024</h2>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 2023 </tag>
            
            <tag> 回顾总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网关金融场景下的大规模实践</title>
      <link href="/2023/12/30/funcdesign/wang-guan-jin-rong-chang-jing-xia-de-da-gui-mo-shi-jian/"/>
      <url>/2023/12/30/funcdesign/wang-guan-jin-rong-chang-jing-xia-de-da-gui-mo-shi-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在近几年，网易数帆轻舟云原生网关在国内头部证券，银行等金融场景进行大规模实践。在落地过程中，有一些典型场景以及一些痛点问题。本章主要聚焦几个真实的场景，分享在金融场景下的云原生网关大规模实践。</p><h2 id="金融场景下的云原生网关"><a href="#金融场景下的云原生网关" class="headerlink" title="金融场景下的云原生网关"></a>金融场景下的云原生网关</h2><h3 id="私有协议扩展"><a href="#私有协议扩展" class="headerlink" title="私有协议扩展"></a>私有协议扩展</h3><p>越来越多的大型证券、银行等金融企业要求核心场景下的协议私有化。一部分原因是从企业角度要求，需要自主可控，另一部分也是证券及银行属于敏感企业，有一定的国家监管层面要求。</p><p>总结下来主要有三个明显的特征需求：</p><ul><li><p>协议定制化</p></li><li><p>入口协议统一</p></li><li><p>多协议转换诉求</p></li></ul><p>针对性的，我们提供两个解决方案，针对入口协议统一，upstream协议多样的场景，通过复用Envoy HTTP Connection Managaer（HCM）提供的标准filter，提炼通用的generic_filter。针对性的扩展私有协议的编解码。这样实现的好处是，可以复用HCM提供的标准filter以及我们在http维度的积累。只需要扩展对应的协议即可以开箱即用的集成40余种插件。实现的架构如下图：</p><p><img src="/../../img/funcDesign/http_qstep_arc.png" alt="http_qstep_arc.png"></p><p>针对多协议互转以及入口协议非HTTP的场景，考虑到协议处理维度的相同点，大部分request,response的rpc架构都有相似的流式处理，路由，可观测等能力。为了简化及抽象协议扩展的复杂度，我们提出设计通用代理（generic_proxy）的架构。主要分析generic_proxy的设计理念及设计目标，提供通用的generic_proxy的编解码的扩展点，定义通用的generic route API。通过generic proxy的实现，旨在为私有协议的扩展提供通用性。其核心设计架构如下图：</p><p><img src="/../../img/funcDesign/qstep_generic_proxy.png" alt="qstep_generic_proxy.png"></p><h3 id="插件扩展能力"><a href="#插件扩展能力" class="headerlink" title="插件扩展能力"></a>插件扩展能力</h3><p>在金融场景下，会有一些业务定制诉求。从敏捷开发角度上考虑，需要支持一定的快速扩展能力，满足业务敏捷开发；同时，由于存在一些敏感合规的要求，业务会要求具备自身定制特殊需求的能力。因此，对云原生网关提出了以下要求：</p><ul><li><p>敏捷扩展</p></li><li><p>插件隔离</p></li><li><p>动态生效</p></li></ul><p>从插件开发人员和插件使用视角，我们抽象插件扩展。形成插件扩展解决方案，流程如下：<br><img src="/../../img/funcDesign/%E6%8F%92%E4%BB%B6%E6%89%A9%E5%B1%95.png" alt="插件扩展.png"></p><p>从设计维度，本着多语言，无侵入，高性能，可视化的原则，我们从以下几方面进行扩展</p><p><img src="/../../img/funcDesign/rider.png" alt="rider.png"></p><h3 id="业务平滑上云"><a href="#业务平滑上云" class="headerlink" title="业务平滑上云"></a>业务平滑上云</h3><p>金融企业作为敏感行业，稳定性压倒一切。因此，在云原生迁移过程一定不是一蹴而就的，对于网关提出，我们如何帮助业务顺利完成云原生改造，协助业务完成平滑迁移。<br><img src="/../../img/funcDesign/%E4%B8%9A%E5%8A%A1%E5%B9%B3%E6%BB%91%E4%B8%8A%E4%BA%91%E7%9A%84%E7%97%9B%E7%82%B9.png" alt="业务平滑上云的痛点.png"><br>迁移过程中，主要有三点痛点：</p><ul><li><p>注册中心复杂</p></li><li><p>服务模型不统一</p></li><li><p>存量网关迁移困难</p></li></ul><p>基于以上痛点，我们从三个方面解决业务平滑上云的痛点，整体架构如下：</p><p><img src="/../../img/funcDesign/%E4%B8%9A%E5%8A%A1%E5%B9%B3%E6%BB%91%E4%B8%8A%E4%BA%91.png" alt="业务平滑上云.png"></p><p>其核心主要有几点：</p><ul><li><p>通过slime mesh-registry模块实现对接多注册中心能力，已经集成zk,eureka,nacos等注册中心。</p></li><li><p>提供丰富的upstream管理及路由能力</p></li><li><p>抽象服务概念，统一k8s svc、静态ip、eureka app以及dubbo interface</p></li><li><p>通过mesh-registry对接多注册中心，同步ServiceEntry资源</p></li><li><p>支持同协议多服务发布，用于流量灰度</p></li><li><p>提供权重分流、版本分流的能力，支持服务金丝雀发布</p></li><li><p>通过sync-service 对接同步不同注册中心的配置数据，支持对接DB、Zookeeper等配置结构化数据，用于业务原网关配置迁移至云原生网关。</p></li></ul><h3 id="全链路灰度"><a href="#全链路灰度" class="headerlink" title="全链路灰度"></a>全链路灰度</h3><p>在实际开发过程中，会存在多版本的发布及上线。复杂的微服务集群，如何能够快速拉起一套全链路的灰度逻辑环境，用于线下全链路测试、生产版本问题定位等是一个比较棘手的事情。在传统的微服务架构下，业务会通过修改自身代码，将一定特征的流量自主路由至不同版本。或者提供一套独立的环境，用于问题复现。总之，两种方式都不够轻量，涉及链路及交付周期等问题。</p><p>因此，全链路灰度的提出，可以将特征请求独立在不同的完全逻辑隔离的运行时环境，能够响应快速迭代、测试、问题复现等场景。其示意图如下：</p><p><img src="/../../img/funcDesign/%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%9C%BA%E6%99%AF.png" alt="全链路灰度场景.png"></p><p>对于云原生网关来说，在全链路灰度场景下，需要关注到三个核心点：</p><ul><li><p>标签同步：能够同步服务实例，支持多注册中心的实例标签同步。</p></li><li><p>条件匹配：能够支持对入口流量根据不同的条件进行流量打标。</p></li><li><p>流量负载：根据不同的流量标签，将流量目的路由到对应标签（颜色）的实例。其流量架构如下：</p></li></ul><p>我们通过slime mesh-registry同步不同注册中心的实例，并通过Header Rewrite能力进行路由匹配条件的重写，将匹配后的信息增加额外的实例颜色标签。这一流程，从处理上为入口流量染色。之后进入核心负载均衡阶段，通过Envoy LB Subset的能力，实现染色路由。完成全链路灰度的第一跳灰度。</p><p><img src="/../../img/funcDesign/%E6%A0%87%E7%AD%BE%E6%9F%93%E8%89%B2.png" alt="标签染色.png"></p><h3 id="网关多租户及隔离性"><a href="#网关多租户及隔离性" class="headerlink" title="网关多租户及隔离性"></a>网关多租户及隔离性</h3><p>租户隔离是在企业网关中非常重要的特性，特别是在金融场景下，例如银行，其下属有众多分行；而在证券企业中，又有应用分级。网关需要具备多租户的配置隔离能力，针对不同系统之间的配置互不影响。</p><p>在传统的网关实现中，只能通过部署不同的网关集群，业务进行不同集群的划分进行强物理隔离。如下图<br><img src="/../../img/funcDesign/%E7%89%A9%E7%90%86%E9%9A%94%E7%A6%BB.png" alt="物理隔离.png"></p><p>但是这种隔离会存在一定的资源浪费，集群之间的流量不均等，有的业务集群流量很低，但是因为其系统的特殊性，政策要求必须对齐进行配置隔离以避免相互影响。我们从Envoy的实现角度，进行优化。Envoy的核心就是代理，其最主要的就是确认流量，并将流量转发到对应的upstream。</p><p>下图是Envoy作为Proxy的核心流程，Envoy会对每一个监听器抽象一层Listener，此Listener可以通过动态的方式扩展并对外暴露。流量通过对应的Listener进入核心L4,L7 Filter后，通过负载均衡策略路由至不同的cluster（即Upstream）</p><p><img src="/../../img/funcDesign/proxy%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png" alt="proxy处理流程.png"></p><p>因此，我们可以通过抽象不同的Listener为逻辑网关，每一个Listener上的配置相互独立。基于此，用户可以选择性的在同一个物理网关集群上抽象不同的逻辑网关，提供不同的网关分组。其逻辑图如下：</p><p><img src="/../../img/funcDesign/%E7%89%A9%E7%90%86%E9%9A%94%E7%A6%BB.png" alt="物理隔离.png"></p><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>在云原生网关的探索上，我觉得还可以从以下三个方面进行后续的演进。</p><ul><li><p>全能力网关建设： 网关逐步被大家作为南北流量的通用性所接纳，不止南北流量，当前网关也会结合编排等能力，多协议转换能力被用于微服务体系。</p></li><li><p>生产级别稳定性持续建设：从细粒度观测 -&gt; 精准化告警 -&gt; 快速恢复 -&gt; 流量无损 四位一体构建监控体系；从事前阶段，提升系统稳定时间为目标，尽可能在不确定环境中降低事故发生率，事中阶段降低系统不稳定时间，事故发生尽可能快速恢复角度持续优化立体化监控，支撑事故发生的问题焦点快速发现。</p></li><li><p>AI赋能：通过沉淀专家经验，结合AI洞察能力，尽量做到问题前置，洞察故障。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云原生网关 </tag>
            
            <tag> Envoy </tag>
            
            <tag> 金融 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网关可观测性建设-(Envoy篇)</title>
      <link href="/2023/09/08/funcdesign/wang-guan-ke-guan-ce-xing-jian-she-envoy-pian-zhang/"/>
      <url>/2023/09/08/funcdesign/wang-guan-ke-guan-ce-xing-jian-she-envoy-pian-zhang/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云原生网关 </tag>
            
            <tag> Envoy </tag>
            
            <tag> 可观测性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全能力网关建设</title>
      <link href="/2023/08/25/funcdesign/quan-neng-li-wang-guan-jian-she/"/>
      <url>/2023/08/25/funcdesign/quan-neng-li-wang-guan-jian-she/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云原生网关 </tag>
            
            <tag> Envoy </tag>
            
            <tag> Hango </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloudGateway云上的设计与改造</title>
      <link href="/2023/07/14/funcdesign/springcloudgateway-yun-shang-de-she-ji-yu-gai-zao/"/>
      <url>/2023/07/14/funcdesign/springcloudgateway-yun-shang-de-she-ji-yu-gai-zao/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloudGateway </tag>
            
            <tag> 云原生网关 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Jmeter的Operator改造</title>
      <link href="/2023/07/14/funcdesign/ji-yu-jmeter-de-operator-gai-zao/"/>
      <url>/2023/07/14/funcdesign/ji-yu-jmeter-de-operator-gai-zao/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Jmeter </tag>
            
            <tag> Operator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开源项目Hango网关项目设计与实践</title>
      <link href="/2023/07/14/funcdesign/kai-yuan-xiang-mu-hango-wang-guan-xiang-mu-she-ji-yu-shi-jian/"/>
      <url>/2023/07/14/funcdesign/kai-yuan-xiang-mu-hango-wang-guan-xiang-mu-she-ji-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Hango 是由网易公司开源的一个基于Envoy构建的高性能、可扩展功能丰富的云原生API网关，本文将从Hango的诞生背景、架构设计、功能特性、大规模落地等方面进行介绍。</p><h2 id="诞生背景"><a href="#诞生背景" class="headerlink" title="诞生背景"></a>诞生背景</h2><p>Envoy起初在网易轻舟是作为服务网格Istio的数据面使用，承担了东西向、南北向全部数据流量的代理、治理与观测职责。随着服务网格在网易内部大规模落地，我们对 Envoy 的功能、性能、扩展性、可观测性等多方面有了全面的研究与实践，也深刻感受到 Envoy 优质的内在品质，及其在云原生时代巨大的发展潜力。<br>于是我们开始尝试基于 Envoy 建设网易的新一代 API 网关，目标是替换网易内部较多业务采用 Java 异步化网关、 Kong 网关，并能够满足业务逐步进入云原生时代的南北向流量治理需求。从结果上看，选型 Envoy 不仅让我们顺利实现了网易 API 网关全面升级，还推动了网易云原生、微服务技术栈整体的统一与向前发展。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p>Hango 基于云原生理念构建，数据面基于Envoy进行扩展，增强插件链，提供 Rider 模块用于自定义插件扩展；控制面组件包括 Slime，Istio，API Plane以及Portal模块。其架构图如下所示：</p><p><img src="/../../img/hango%E8%AE%BE%E8%AE%A1%E6%9E%B6%E6%9E%84.png" alt="hango设计架构.png"></p><p>组件说明：</p><ul><li>Envoy: Hango网关的数据面流量入口，通过XDS模式获取Istiod配置</li><li>Istiod: Hango网关对接Envoy的适配层，监听指定CRD并通过XDS向Envoy下发配置</li><li>Slime: 提供指定Slime CRD，以支持Hango支持插件、限流、服务发现等特性</li><li>Hango Portal: Hango对接前端界面的Web层模块，存储Hango业务数据</li><li>Hango Api-plane: 对接Hango Portal，管理Hango CR声明周期</li><li>Hango UI: Hango前端界面，提供Hango配置、观测、调试等功能</li></ul><p>Hango UI将Envoy的功能进行了可视化的配置，与Hango Portal进行交互，Hango Portal将业务信息存于DB，并调用Hango Api-plane生成服务路由相应的CR，Istiod在Watch到API-plane生成的CR会通过XDS协议下发给数据面Envoy，Envoy将配置信息应用到数据面。<br>对于插件的能力的扩展，我们使用自定义的CRD(EnvoyPlugin)来承载插件信息，Slime模块监听Slime CRD，将Slime CRD转换为EnvoyFilter的插件配置对数据面envoy进行扩展。</p><p>下面是Hango的数据流向：</p><h2 id="功能特性"><a href="#功能特性" class="headerlink" title="功能特性"></a>功能特性</h2><h3 id="虚拟网关"><a href="#虚拟网关" class="headerlink" title="虚拟网关"></a>虚拟网关</h3><h4 id="虚拟网关-1"><a href="#虚拟网关-1" class="headerlink" title="虚拟网关"></a>虚拟网关</h4><p>在L7层通用网关中，一组Envoy网关集群可以表现出不同的形态，如API网关、L7负载均衡、ingress。网关集群在部署时已确定的一组网关实例，称之为网关，不同表现形态称之为虚拟网关。 依托于网关，可通过不同场景进行配置。网关数据层采用Envoy作为底层代理，通过不同的Listener配置开放Envoy不同的监听端口，对应不同形态的虚拟网关。在L7层通用网关中，一组Envoy网关集群可以表现出不同的形态，如API网关、L7负载均衡、ingress。网关集群在部署时已确定的一组网关实例，称之为网关，不同表现形态称之为虚拟网关。 依托于网关，可通过不同场景进行配置。网关数据层采用Envoy作为底层代理，通过不同的Listener配置开放Envoy不同的监听端口，对应不同形态的虚拟网关。</p><p>以下为虚拟网关的功能架构图：<br><img src="/../../img/%E8%99%9A%E6%8B%9F%E7%BD%91%E5%85%B3%E5%8A%9F%E8%83%BD%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="虚拟网关功能架构图.png"></p><p>一组Envoy网关实例部署在Kubernetes集群中，用户通过配置服务在网关生成Listener(虚拟网关)，Listener(虚拟网关)监听对应的服务，当有请求访问该监听端口时，进行流量转发。</p><h4 id="Kubernetes-Gateway"><a href="#Kubernetes-Gateway" class="headerlink" title="Kubernetes Gateway"></a>Kubernetes Gateway</h4><p>Gateway API作为Kubernetes入口网关的最新成果，得到行业的广泛支持。它代表了Ingress功能的一个父集，定义了一系列以Gateway资源为中心的资源集合。与Ingress类似，Kubernetes 中没有内置Gateway API默认实现，需要依赖基础设施商提供Gateway Class。 </p><p>官方文档见：<a href="https://gateway-api.sigs.k8s.io/">https://gateway-api.sigs.k8s.io</a></p><p>作为Ingress资源的升级，Gateway API提供了一系列治理能力更强、表达性更优、可扩展性更高的资源集合，其中GatewayClass、Gateway和HTTPRoute已经进入Beta阶段，其他CRD还处于实验阶段。 此处只需要关注Gateway和xRoute资源，详细的API定义可参考Gateway API</p><p>以下图片为Gateway API的相关组件：<br><img src="/../../img/Gateway%20API%E7%9B%B8%E5%85%B3%E7%BB%84%E4%BB%B6.png" alt="Gateway API相关组件.png"></p><ul><li><p>GatewayClass：GatewayClass是由基础架构提供商定义的集群范围的资源，该资源用于指定对应的Gateway Controller。目前已实现的Gateway Controller的产品包括Envoy Gateway（beta）、Istio（beta）、Kong（beta）等，详情可参考Gateway Controller</p></li><li><p>Gateway： 核心网关资源，主要规范了以下三部分内容：</p><ul><li><p>Listeners：网关监听器列表，每个监听器都代表了一组主机名、端口、协议配置。</p></li><li><p>GatewayClassName：用于指定生效的GatewayClass。</p></li><li><p>Address：定义网关代理的请求地址。</p></li></ul></li><li><p>xRoute： 代表需要不同特性协议的路由资源，每种协议路由都有特定的语义，这种模式具有较好的扩展性，例如可以定义DubboRoute、gRPCRoute等。每种资源都定义了基本的匹配、过滤和路由规则，这些规则只有被绑定到相应的Gateway资源上才可以生效。目前只有HTTPRoute进入Beta阶段。</p></li></ul><p>以下为Kubernetes Gateway在hango的功能实现架构图，Gateway API在控制台被创建之后会被开源的Istio控制面将Gateway API对象转换为Istio API对象，最终下发至Envoy数据面。 hango在此基础之上对HttpRoute做了插件上的增强，提供了更多丰富的插件能力。</p><p><img src="/../../img/Kubernetes%20Gateway%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Kubernetes Gateway技术架构图.png"></p><h4 id="Kubernetes-Ingress"><a href="#Kubernetes-Ingress" class="headerlink" title="Kubernetes Ingress"></a>Kubernetes Ingress</h4><p>Ingress是K8s生态中定义流量入口的一种资源，但其只是个规则，仅创建Ingress资源本身是没有任何效果的，想让其生效，就需要有一个Ingress Controller去监听K8s中的ingress资源， 并对这些资源进行规则解析，转换为其数据面中的代理规则，并由数据面来进行流量转发。当前K8s默认的Ingress Controller实现是Nginx，本次方案描述如何将通用网关纳管Ingress的流量。</p><p>Ingress资源是对集群中服务的外部访问进行管理的 API 对象，可以将集群内部服务通过HTTP或HTTPS暴露出去，流量路由规则由Ingress资源定义。<br><img src="/../../img/Ingress.png" alt="Ingress.png"></p><p>目前只支持Ingress v1版本流量纳管，v1版本的Ingress资源定义如下：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token comment">##ingress v1</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress<span class="token key atrule">metadata</span><span class="token punctuation">:</span>   <span class="token key atrule">annotations</span><span class="token punctuation">:</span>      <span class="token key atrule">kubectl.kubernetes.io/last-applied-configuration</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">         {"apiVersion":"networking.k8s.io/v1","kind":"Ingress","metadata":{"annotations":{"kubernetes.io/ingress.class":"istio"},"name":"test","namespace":"hango-system"},"spec":{"ingressClassName":"istio","rules":[{"http":{"paths":[{"backend":{"service":{"name":"istio-e2e-app","port":{"number":80}}},"path":"/get","pathType":"Prefix"}]}}]}}</span>      <span class="token key atrule">kubernetes.io/ingress.class</span><span class="token punctuation">:</span> hango      <span class="token key atrule">skiff.netease.com/project</span><span class="token punctuation">:</span> hango   <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token string">"2023-07-14T07:33:17Z"</span>   <span class="token key atrule">generation</span><span class="token punctuation">:</span> <span class="token number">7</span>   <span class="token key atrule">name</span><span class="token punctuation">:</span> test   <span class="token key atrule">namespace</span><span class="token punctuation">:</span> hango<span class="token punctuation">-</span>system   <span class="token key atrule">resourceVersion</span><span class="token punctuation">:</span> <span class="token string">"9320368"</span>   <span class="token key atrule">labels</span><span class="token punctuation">:</span>     <span class="token key atrule">istio.io/rev</span><span class="token punctuation">:</span> gw<span class="token punctuation">-</span><span class="token number">1.12</span>   <span class="token key atrule">uid</span><span class="token punctuation">:</span> 1ba7e839<span class="token punctuation">-</span>da43<span class="token punctuation">-</span>4c00<span class="token punctuation">-</span>afeb<span class="token punctuation">-</span>85d173911003<span class="token key atrule">spec</span><span class="token punctuation">:</span>   <span class="token key atrule">rules</span><span class="token punctuation">:</span>     <span class="token punctuation">-</span> <span class="token key atrule">http</span><span class="token punctuation">:</span>        <span class="token key atrule">paths</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">backend</span><span class="token punctuation">:</span>              <span class="token key atrule">service</span><span class="token punctuation">:</span>              <span class="token key atrule">name</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>e2e<span class="token punctuation">-</span>app              <span class="token key atrule">port</span><span class="token punctuation">:</span>              <span class="token key atrule">number</span><span class="token punctuation">:</span> <span class="token number">80</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /get            <span class="token key atrule">pathType</span><span class="token punctuation">:</span> Prefix<span class="token key atrule">status</span><span class="token punctuation">:</span>  <span class="token key atrule">loadBalancer</span><span class="token punctuation">:</span>    <span class="token key atrule">ingress</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">ip</span><span class="token punctuation">:</span> xxx.xxx.xxx.xxx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Ingress v1版本的资源定义中，主要包含以下几个重要注解：</p><ul><li>istio.io/rev：指定网关版本，目前只支持gw-1.12</li><li>kubernetes.io/ingress.class：指定ingress的class，目前固定为hango</li><li>skiff.netease.com/project：指定项目ID，目前固定为hango</li><li>spec.rules：指定ingress的路由规则，目前只支持http协议</li><li>status.loadBalancer.ingress：指定ingress的访问地址<h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3>目前Istio1.8之后只保留了对接Kubernetes注册中心的逻辑，但是在实际生产实践中，我们发现大量用户只是将Kubernetes作为部署和管理的平台，服务信息依旧注册在第三方服务注册中心，如Nacos和Zookeeper，因此我们必须解决对接第三方注册中心的问题，以满足用户的需求。<br>我们采用的方案是网易开源的slime组件作为服务发现的适配层，slime组件支持对接多种第三方注册中心，如Nacos、Zookeeper、Eureka、Consul等，同时支持对接Kubernetes注册中心，将服务发现的逻辑统一抽象为slime CRD，通过slime CRD将服务发现的逻辑下发至Envoy数据面。</li></ul><p>slime组件的相关介绍见：<a href="https://slime-io.github.io/blog/Slime%E5%8C%96%E8%A7%A3%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E5%85%BC%E5%AE%B9%E4%B9%8B%E7%97%9B/">Slime化解服务网格多注册中心兼容之痛</a></p><h3 id="可观测"><a href="#可观测" class="headerlink" title="可观测"></a>可观测</h3><p>基于Envoy良好的观测性，Hango网关在网易集团内部进行规模落地过程中，结合服务网格场景，提供丰富的观测能力，整体架构如下</p><ul><li>日志</li></ul><p>Envoy中事件的详细记录，Hango网关基于Envoy进行数据面扩展，提供了灵活易配置的AccessLog，支持自定义格式，自定义顾虑规则以及输出。</p><p>基于filebeat以及elastic的能力，构建一体化日志审计平台。</p><ul><li>监控</li></ul><p>基于Envoy cluster mertic等信息，利用Promethues构建网关/服务等多维度指标体系。同时，针对网关容器化部署模式，基于Kubernetes 容器对应的 metrics构建容器维度的指标监控，涵盖CPU/内存/带宽等多维度监控。</p><ul><li>链路追踪</li></ul><p>基于Envoy开箱即用的多种tracing接入，拓展力强，目前已完成线上SkyWalking等tracing的接入。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>Hango作为一名开源领域的新生儿，我们会秉承拥抱云原生的理念，继续跟进Istio/Envoy的演进，发挥更大的领域价值。下一阶段，我们会在多语言扩展，LB融合等多场景进行发力，也期待更多关注云原生、API网关的同学能够加入Hango开源社区建设。</p><p>诚挚的欢迎大家关注Hango Gateway</p>]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云原生网关 </tag>
            
            <tag> Envoy </tag>
            
            <tag> 网易 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网关两地三中心的实践与总结</title>
      <link href="/2023/05/16/funcdesign/wang-guan-liang-di-san-zhong-xin-de-shi-jian-yu-zong-jie/"/>
      <url>/2023/05/16/funcdesign/wang-guan-liang-di-san-zhong-xin-de-shi-jian-yu-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="轻舟网关两地三中心整体技术方案"><a href="#轻舟网关两地三中心整体技术方案" class="headerlink" title="轻舟网关两地三中心整体技术方案"></a>轻舟网关两地三中心整体技术方案</h1><h2 id="背景概述"><a href="#背景概述" class="headerlink" title="背景概述"></a>背景概述</h2><p>轻舟云原生网关作为轻舟平台核心组件，负责承接集群内外的所有入口流量，并对其进行转发和治理。而两地三中心作为一种容灾能力相对比较完整的架构，有助于提升产品的稳定性，具备业务容灾能力。因此本次方案将实现网关产品的两地三中心能力，并具备区域路由优先和多集群服务发现等产品能力。</p><h2 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h2><h3 id="功能需求"><a href="#功能需求" class="headerlink" title="功能需求"></a>功能需求</h3><ul><li><p>区域路由优先</p><p>网关基于区域匹配信息进行流量优先级划分。</p></li><li><p>多集群服务发现</p><p>控制面能够发现多集群中的服务实例。</p></li></ul><h3 id="非功能需求"><a href="#非功能需求" class="headerlink" title="非功能需求"></a>非功能需求</h3><ul><li><p>多集群配置下发</p><p>API Plane支持下发配置到多个集群中。</p></li><li><p>数据一致性保障</p><p>通过定时补偿任务达到数据最终一致性的目标</p></li><li><p>基于K8s Informer缓存资源数据</p><p>API Plane通过K8s Informer机制缓存资源数据，避免通过k8s api server全量拉取资源。</p></li></ul><h2 id="目标和非目标"><a href="#目标和非目标" class="headerlink" title="目标和非目标"></a>目标和非目标</h2><p><strong>目标</strong>：<br>通过阅读该文档，明白网关两地三中心项目的设计思路及方案，并能通过该方案进行相关落地。</p><p><strong>非目标</strong>：<br>阅读人员需要熟悉网关整体架构，主要面向开发和运维人员。</p><h2 id="系统指标"><a href="#系统指标" class="headerlink" title="系统指标"></a>系统指标</h2><p><strong>功能指标</strong></p><p>实现两地三中心多集群部署，保障每个集群的网关代理组件的无状态性。</p><p>提供区域路由优先的产品能力，保证流量优先转发到当前集群。</p><p>提供多集群服务发现能力，实现多集群服务的统一纳管。</p><p>提供多集群配置下发能力，保证多集群网关配置的一致性。</p><p>支持灵活部署，资源有限的场景下，可以支持数据面高可用，控制面单集群部署。</p><p><strong>性能指标</strong></p><p>保证两地三中心部署模式下，流量转发延时和单集群延时保持一致。</p><p>多集群下每个网关都可以承接所有流量，QPS理论上可以n倍于单集群网关。</p><h2 id="功能架构设计"><a href="#功能架构设计" class="headerlink" title="功能架构设计"></a>功能架构设计</h2><h3 id="区域路由优先"><a href="#区域路由优先" class="headerlink" title="区域路由优先"></a>区域路由优先</h3><p>两地三中心模式下，可以通过地区（region）、可用区（zone）、集群(cluster)三元组定义网关和业务服务实例的地域信息。</p><ul><li>地区：代表较大的地理区域，例如杭州。一个地区通常包含许多可用区域。通过node标签mlha.skiff.netease.com/region确定服务的地区。</li><li>区域：代表一个可用区，例如滨江。一个区域内通常会部署多个集群。通过node标签mlha.skiff.netease.com/zone确定服务的可用区。</li><li>集群：代表服务所在的K8S集群，通过node标签mlha.skiff.netease.com/cluster确定服务的可用区。</li></ul><p>例如：【杭州、滨江、集群A】，【杭州、余杭、集群B】</p><p><img src="/../../img/funcDesign/%E5%8C%BA%E5%9F%9F%E8%B7%AF%E7%94%B1%E4%BC%98%E5%85%88.png" alt="区域路由优先.png"></p><p>若开启地域区域路由优先功能，网关代理（Envoy）会基于如下规则对流量进行转发：</p><p>1.优先将请求转发到当前集群中的服务。</p><p>2.若网关所在集群的服务不可用，则将流量转发到当前可用区中其他集群的服务。</p><p>3.若网关所在可用区的服务都不可用，则将流量转发到当前区域中其他集群的服务。</p><p>4.若网关所在区域的服务都不可用，则将流量转发到其他区域中的服务。</p><h2 id="技术架构设计"><a href="#技术架构设计" class="headerlink" title="技术架构设计"></a>技术架构设计</h2><p>网关两地三中心方案是在现有网关组件的基础上，通过多集群配置下发与监听、多集群服务发现等技术实现网关集群无状态水平扩展，每个集群都包含完整的网关组件。</p><h3 id="整体技术架构"><a href="#整体技术架构" class="headerlink" title="整体技术架构"></a>整体技术架构</h3><p><img src="/../../img/funcDesign/%E7%BD%91%E5%85%B3%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84.png" alt="网关总体架构.png"></p><p>网关的整体架构如图所示，主要可以划分为如下4部分：</p><ul><li><p>控制面组件</p><p>控制面组件负责配置管理和服务发现，具体包括如下组件：</p><p>Front：前端组件，负责轻舟网关的可视化展示。</p><p>GPortal：轻舟网关控制台，和前端组件交互，负责配置管理和可视化。</p><p>API Plane：资源管理组件，和K8s api server交互，负责配置资源（CRD）的创建。</p><p>Istio Pilot：网关团队基于Istio开源控制面Pilot进行增强，主要负责与数据面Envoy的交互，包括服务以及配置等信息通过xDS协议与数据面进行交互。</p><p>Mesh Registry：服务发现组件，通过MCP协议从注册中心获取服务实例，支持的注册中心包括Nacos、Kubernetes、Eureka、Zookeeper。</p></li><li><p>数据面组件</p><p>网关代理组件（Envoy Proxy），负责流量治理和转发，当网关使用集群限流功能时，需要额外部署Rate Limit组件。</p></li></ul><h3 id="多集群架构"><a href="#多集群架构" class="headerlink" title="多集群架构"></a>多集群架构</h3><p><img src="/../../img/funcDesign/%E7%BD%91%E5%85%B3%E5%A4%9A%E9%9B%86%E7%BE%A4.png" alt="网关多集群.png"></p><p>网关多集群水平扩展后如图所示，每个集群都包含完整数据面和控制面组件，其中数据面组件包含完整的服务和配置信息，可以完整承接网关流量，上层通过Nginx进行流量负载均衡。</p><h3 id="持久化数据依赖说明"><a href="#持久化数据依赖说明" class="headerlink" title="持久化数据依赖说明"></a>持久化数据依赖说明</h3><p>网关依赖的持久化组件如下：</p><p>Mysql（强依赖）：持久化流量治理相关配置，用于视图管理和资源下发。</p><p>Etcd（强依赖）：K8s Api Server依赖组件，每个K8s集群都单独维护一个Etcd，用于存储相关Istio配置资源，Istio监听Etcd资源并下发配置给挖宝方法代理Envoy。</p><p>Redis（可选）：用于缓存和限流插件。</p><p>Prometheus(可选)：用于指标数据上报和统计。</p><h2 id="部署架构设计"><a href="#部署架构设计" class="headerlink" title="部署架构设计"></a>部署架构设计</h2><p><img src="/../../img/funcDesign/%E4%B8%A4%E5%9C%B0%E4%B8%89%E4%B8%AD%E5%BF%83%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84.png" alt="两地三中心部署架构.png"></p><p>网关两地三中心部署方案如上所示，其中区域1为当前提供服务的区域，通过SLB将流量负载均衡到可用区A和B中；区域2为异地区域，区域1中的mysql配置数据会定时同步到可用区C的灾备中间件中。当进行故障切换时，需要执行如下两步骤：</p><p>1.同步最新的mysql配置数据到灾备数据库中。</p><p>2.基于同步后的灾备数据，全量发布网关资源（服务、路由、插件）。</p><h2 id="容灾方案设计"><a href="#容灾方案设计" class="headerlink" title="容灾方案设计"></a>容灾方案设计</h2><h3 id="同城单机房故障应对及影响"><a href="#同城单机房故障应对及影响" class="headerlink" title="同城单机房故障应对及影响"></a>同城单机房故障应对及影响</h3><h4 id="平台侧"><a href="#平台侧" class="headerlink" title="平台侧"></a>平台侧</h4><p>SLB感知机房故障，自动将流量切换到同城可用机房，不会对流量产生影响。</p><h4 id="业务侧"><a href="#业务侧" class="headerlink" title="业务侧"></a>业务侧</h4><p>部署在本机房中的业务服务将无法接受流量，因此推荐用户将服务分散部署在多个集群中，避免机房故障导致服务实例不可用。</p><h3 id="同城双机房故障应对及影响"><a href="#同城双机房故障应对及影响" class="headerlink" title="同城双机房故障应对及影响"></a>同城双机房故障应对及影响</h3><h4 id="平台侧-1"><a href="#平台侧-1" class="headerlink" title="平台侧"></a>平台侧</h4><p>故障期间会导致网关不可用，需要人为切换到灾备机房，切换后需要同步最新数据到灾备数据库中，并进行全量资源发布后可正常提供服务。</p><h4 id="业务侧-1"><a href="#业务侧-1" class="headerlink" title="业务侧"></a>业务侧</h4><p>当前可用区中的服务将不可用，灾备机房中的服务可以正常接受流量。</p><h2 id="关键方案设计"><a href="#关键方案设计" class="headerlink" title="关键方案设计"></a>关键方案设计</h2><h3 id="多集群读写"><a href="#多集群读写" class="headerlink" title="多集群读写"></a>多集群读写</h3><p><img src="/../../img/funcDesign/API%20Plane%E5%A4%9A%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E4%B8%8B%E5%8F%91.png" alt="API Plane多集群配置下发.png"></p><p>在网关下发配置流程中，GPortal负责落库，API Plane负责下发配置到K8S API Server。在两地三中心场景下，API Plane还需要提供多集群配置下发的能力。在具体实现层面，api plane需要指定主机群和从集群配置，其中主机群为当前集群，从集群为其他集群。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">clusters</span><span class="token punctuation">:</span>     <span class="token key atrule">master</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-api-server</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">cert-data</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">key-data</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">ca-data</span><span class="token punctuation">:</span> <span class="token string">""</span>    <span class="token key atrule">test209</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-api-server</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">cert-data</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">key-data</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">ca-data</span><span class="token punctuation">:</span> <span class="token string">""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>读操作</p><p>对于读操作API Plane只从主集群中读取配置，若主集群crash，会导致pass平台视图展示失败，需要通过Nginx进行切换；</p></li><li><p>写操作</p></li></ul><p>​        写操作包含GPortal落库和API Plane多集群配置下发两个操作。本方案优先API Plane下发配置，并且只要主集群配置下发成功，就返回成功，GPortal执行DB操作。对于从集群则通过子线程异步进行更新，若更新失败导致数据不一致，则需要通过定时补偿任务进行修正，保证数据的最终一致性。</p><h3 id="定时补偿任务"><a href="#定时补偿任务" class="headerlink" title="定时补偿任务"></a>定时补偿任务</h3><p>本方案通过API Plane执行定时补偿任务对数据进行修正。其中每个API Plane组件都会校验本集群中CR和DB数据是否一致，若出现不一致，则以DB数据为准更新本集群CR。针对资源类型的不同，API Plane采用版本号校验和内容校验两种方式。</p><h4 id="版本号校验："><a href="#版本号校验：" class="headerlink" title="版本号校验："></a>版本号校验：</h4><p>通过对比DB中版本号和CR中版本号是否一致来判断配置是否成功下发，优先采用该方式，涉及的资源包括Destination Rule、Virtual Service和GatewayPlugin。</p><p>具体步骤：</p><p>1.相关表添加版本号标识符version字段，数据创建时初始化为0，之后每次修改都自增。</p><p>2.API Plane下发配置时需要带上版本号，映射到资源metadata中的hango.data.version字段。</p><p>3.定时触发校验校验任务比较两种的版本号是否一致，若版本号一致，则表明配置正常下发了；否则需要基于DB中的数据进行修正，修正场景如下：</p><table><thead><tr><th>场景</th><th>数据库</th><th>CR</th><th>操作</th></tr></thead><tbody><tr><td>配置未更新</td><td>version=2</td><td>skiff.nsf.data.version=1</td><td>更新CR</td></tr><tr><td>配置未创建</td><td>version=2，enable=true</td><td>NULL</td><td>创建CR</td></tr><tr><td>禁用相关功能</td><td>enable=false</td><td>NULL</td><td>不操作</td></tr><tr><td>数据库更新失败</td><td>version=2</td><td>skiff.nsf.data.version=3</td><td>更新CR</td></tr><tr><td>配置未删除</td><td>NULL</td><td>skiff.nsf.data.version=1</td><td>删除CR</td></tr></tbody></table><p>上述为版本号校验的基本步骤，存在下面三种情况需要特殊处理：</p><ul><li><p>双表映射相同资源</p><p>解释：两个表映射到相同资源</p><p>场景：</p><p>1.apigw_service_proxy和apigw_envoy_health_check_rule映射Destination Rule资源。</p><p>2.apigw_route_rule_proxy和apigw_gportal_dubbo_info映射Virtual Service资源。</p><p>解决方案：只在主映射表（apigw_service_proxy， apigw_route_rule_proxy）添加版本号，从映射表更新配置时，需要更新主映射表的版本号。</p></li></ul><p><img src="/../../img/funcDesign/%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5version.png" alt="健康检查version.png"></p><ul><li><p>同表多记录映射相同资源</p><p>解释：同一个表中的多条记录映射到相同场景</p><p>场景：路由级插件apigw_envoy_plugin_binding表中相同路由下的多个插件映射到同一个GatewayPlugin资源</p><p>解决方案：多条记录维护相同的版本号，具体步骤如下：</p></li></ul><p><img src="/../../img/funcDesign/%E9%A1%B9%E7%9B%AE%E7%BA%A7%E6%8F%92%E4%BB%B6%E6%B5%81%E7%A8%8B.png" alt="项目级插件流程.png"></p><h4 id="内容校验："><a href="#内容校验：" class="headerlink" title="内容校验："></a>内容校验：</h4><p>直接比较CR资源和DB中的详细内容，该方式主要应用于内容相对简单的场景，具体指Service Entry资源。该资源只包含静态服务地址信息。</p><h3 id="API-Plane资源监听"><a href="#API-Plane资源监听" class="headerlink" title="API Plane资源监听"></a>API Plane资源监听</h3><p>定时补偿任务需要全量拉取K8s 资源进行校验，数据量过大时查询较慢（1w+数据需要查询3分钟），导致api server压力过大，同时导致并发问题影响补偿任务的正确性。为了解决上述问题，本方案通过K8s Informer机制实现资源缓存。</p><p><img src="/../../img/funcDesign/K8s%20informer.png" alt="K8s informer.png"></p><p>K8s Infomer机制的核心是List/Watch，Api plane 在全量拉取资源时会使用Informer中的Lister()方法，从本地缓存中（store）获取，而非直接请求Kubernetes API。而本地缓存则通过watch机制进行实时更新。具体原理可参考<a href="https://houmin.cc/posts/1f0eb2ff/">Kubernetes Informer详解</a> 。</p><h3 id="Istio多集群服务发现"><a href="#Istio多集群服务发现" class="headerlink" title="Istio多集群服务发现"></a>Istio多集群服务发现</h3><p><img src="/../../img/funcDesign/Istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%9B%91%E5%90%AC.png" alt="Istio多集群监听.png"><br>该部分主要基于Istio提供的<a href="https://istio.io/latest/zh/docs/setup/install/multicluster/multi-primary/">多主架构部署方案</a>实现，需要保证多集群处于单一的互通网络，任意负载间网络可达。该方案中每个集群中的Istio都会监听所有集群K8s API服务器的服务端点，从而实现获取所有集群中的服务实例。</p><p>Mesh Registry也具备多集群监听的能力，实现方式参照Istio，因此不进行额外说明。</p><h2 id="监控、告警及运维"><a href="#监控、告警及运维" class="headerlink" title="监控、告警及运维"></a>监控、告警及运维</h2><p>无</p><h2 id="遗留或待解决问题"><a href="#遗留或待解决问题" class="headerlink" title="遗留或待解决问题"></a>遗留或待解决问题</h2><p>无</p><h2 id="讨论及更新记录"><a href="#讨论及更新记录" class="headerlink" title="讨论及更新记录"></a>讨论及更新记录</h2><p>无</p>]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云原生网关 </tag>
            
            <tag> 两地三中心 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Hango网关中gportal和api-plane组件合并实施方案</title>
      <link href="/2023/03/12/funcdesign/guan-yu-hango-wang-guan-zhong-gportal-he-api-plane-zu-jian-he-bing-shi-shi-fang-an/"/>
      <url>/2023/03/12/funcdesign/guan-yu-hango-wang-guan-zhong-gportal-he-api-plane-zu-jian-he-bing-shi-shi-fang-an/</url>
      
        <content type="html"><![CDATA[<h1 id="组件合并实施方案"><a href="#组件合并实施方案" class="headerlink" title="组件合并实施方案"></a>组件合并实施方案</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>网关为了降低部署成本，减少运维代价,现需要将gportal和api-plane的组件在部署的时候进行合并，精简部署的组件。以下将围绕实施方案以及需要考虑的相关问题进行展开。<br>方案 不涉及到代码模块的合并，在K8s基座内部的服务模块合并方案不外乎两种，一是将portal和api-plane部署在pod的同一容器进程内，二是将portal和api-plane放在同一pod的不同容器中。<br>但是第一种方案将两个服务放在同一个容器中既不符合Kubernetes的最佳实践，同时也存在诸多问题，诸如：</p><ul><li><p>它们的部署和管理变得更加复杂，对于不同的服务可能需要不同的配置和更新策略，这会增加容器的复杂度和维护成本。</p></li><li><p>共享同一个资源池，这可能会导致它们之间的资源争用或者不均衡，难以对它们进行单独的扩展和调整。</p></li><li><p>共享同一个进程空间和文件系统，缺乏隔离性，这可能会导致它们之间的冲突或干扰等等。<br>而将gportal和api-plane合并至同一pod的不同容器中可以最大复用原先各自服务的deploy里的配置内容，且不需要更改镜像的配置信息，可以较小工作量完成组件的聚合。<br>基于以上诸多原因我们采用第二种方案将gportal和api-plane合并至同一pod的不同容器中。</p><h2 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h2><p>gportal和api-plane合并至同一pod的不同容器中同时也需要考虑以下问题：</p></li><li><p>资源占用增加<br>原先将其分开部署在不同的deploy中，我们可以按需控制gportal和api-plane各自的副本数，但现在两个服务的容器数量比为<code>1:1</code>，资源开销将有增加。</p></li><li><p>gportal和api-plane之间的通信问题：</p><p>1、api-plane的服务地址是存储在DB的hango_gateway表中，我们可以通过更改表数据来指定api-plane的服务地址。</p><p>2、同一pod的不同容器共享同一个进程空间和网络命名空间，这意味着它们将共享相同的主机名、IP地址和端口号。我们可以指定api-plane的服务地址为localhost进行调用。</p></li><li><p>健康检查<br>我们在做组件合并的过程中会为各个容器服务配置健康检查， 当pod内所有容器的健康检查都成功时，才会将流量路由到Pod，这就要求gportal和api-plane服务必须同时可用。</p><h2 id="如何兼容多集群纳管"><a href="#如何兼容多集群纳管" class="headerlink" title="如何兼容多集群纳管"></a>如何兼容多集群纳管</h2><p>现阶段网关是依靠于gportal调用不同环境乃至不同集群的api-plane地址实现多网关的纳管，每个api-plane的地址所指向的上游集群都享有同一份网关资源。<br><img src="/../../img/funcDesign/%E7%8E%B0%E9%98%B6%E6%AE%B5%E5%A4%9A%E7%BD%91%E5%85%B3%E7%BA%B3%E7%AE%A1.png" alt="现阶段多网关纳管.png"><br>如将部署架构下api-plane和gportal合为同一个pod中，两者同处于管控集群，意味着原先多网关的纳管能力需要从gportal下沉到 api-plane服务中，由api-plane为共享同一份资源的计算集群进行分组。下面我们会以较多篇幅讨论多集群纳管的方案</p></li></ul><h3 id="多集群纳管方案一："><a href="#多集群纳管方案一：" class="headerlink" title="多集群纳管方案一："></a>多集群纳管方案一：</h3><p>由前文我们了解到在新的部署架构中，gportal和api-plane同处于管控集群，我们需要将原先DB存储api-plane服务的地址指向改为 localhost，并且表并且gportal所有对于api-plane的调用都需要传递共享该资源的集群分组标识，由api-plane根据根据分组标识筛选出对应计算集群的信息对其资源进行操作。</p><p><img src="/../../img/funcDesign/%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%BA%B3%E7%AE%A1%E6%96%B9%E6%A1%88%E4%B8%80.png" alt="多集群纳管方案一.png"><br>api-plane配置文件新增了对相同副本的集群进行了分组</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">k8s</span><span class="token punctuation">:</span>  <span class="token key atrule">groups</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">group</span> <span class="token punctuation">:</span> gateway1      <span class="token key atrule">clusters</span><span class="token punctuation">:</span>        <span class="token key atrule">master</span><span class="token punctuation">:</span>          <span class="token key atrule">k8s-api-server</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">cert-data</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">key-data</span><span class="token punctuation">:</span> <span class="token string">""</span>        <span class="token key atrule">slave1</span><span class="token punctuation">:</span>          <span class="token key atrule">k8s-api-server</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">cert-data</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">key-data</span><span class="token punctuation">:</span> <span class="token string">""</span>        <span class="token key atrule">slave2</span><span class="token punctuation">:</span>          <span class="token key atrule">k8s-api-server</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">cert-data</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">key-data</span><span class="token punctuation">:</span> <span class="token string">""</span>    <span class="token punctuation">-</span> <span class="token key atrule">group</span> <span class="token punctuation">:</span> gateway2      <span class="token key atrule">clusters</span><span class="token punctuation">:</span>        <span class="token key atrule">master</span><span class="token punctuation">:</span>          <span class="token key atrule">k8s-api-server</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">cert-data</span><span class="token punctuation">:</span> <span class="token string">""</span>          <span class="token key atrule">key-data</span><span class="token punctuation">:</span> <span class="token string">""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>需要改造内容</p><p>1、api-plane服务deploy、ConfigMap 的yaml文件内容迁移至gportal的deploy containers节点下</p><p>2、DB的<code>hango_gateway</code>表中，我们更改表数据来指定api-plane的服务地址为localhost。</p><p>3、chart场景化抽象</p><p>4、gportal对api-plane发送的请求都需要携带group信息</p><p>5、api-plane的ConfigMap中配置多集群的api-server地址，服务读取ConfigMap初始化集群客户端，并根据group进行客户端缓存</p><p>6、需要操作api-server时，从请求中获取group信息，再从缓存中根据group信息获取Kubernetes 客户端</p><h3 id="多集群纳管方案二："><a href="#多集群纳管方案二：" class="headerlink" title="多集群纳管方案二："></a>多集群纳管方案二：</h3><p>根据前文内容，若gportal和api-plane位于同一管控集群中，那么api-plane就需要承担管理多集群配置下发的责任，这将使得API-plane相关配置下发的实现变得更为复杂。但是，我们可以稍微思考一下，如果将gportal和api-plane都部署在计算集群中，也可以在保持现有部署架构的情况下实现多集群纳管。</p><p><img src="/../../img/funcDesign/%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%BA%B3%E7%AE%A1%E6%96%B9%E6%A1%88%E4%BA%8C.png" alt="多集群纳管方案二.png"></p><h3 id="多集群纳管方案三："><a href="#多集群纳管方案三：" class="headerlink" title="多集群纳管方案三："></a>多集群纳管方案三：</h3><p>以上都是改动比较大的改造，我们也可以不用走一刀切的策略。我们可以对不同场景的chart进行抽象。<br>如果数据面和管控面都处于同一个集群下，那么我们可以考虑portal和api-plane组件在deploy中进行合并。</p><p><img src="/../../img/funcDesign/%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%BA%B3%E7%AE%A1%E6%96%B9%E6%A1%88%E4%B8%89-1.png" alt="多集群纳管方案三-1.png"><br>如果数据面和管控面不在同一个集群下，多集群还是按照现在portal和api-plane组件分离的方式：</p><p><img src="/../../img/funcDesign/%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%BA%B3%E7%AE%A1%E6%96%B9%E6%A1%88%E4%B8%89-2.png" alt="多集群纳管方案三-2.png"></p><h2 id="后续长期规划"><a href="#后续长期规划" class="headerlink" title="后续长期规划"></a>后续长期规划</h2><p>尽管gportal和api-plane的服务职责不同，但它们都属于Java服务的体系都由网关团队进行开发，且api-plane仅被gportal服务所调用，现无论它们在同一pod还是分别拆分到计算管控集群，只要它们作为两个服务存在，就会造成资源损耗。因此，我们可以考虑将它们合并为同一个工程的不同模块，按职责划分模块间关系。下图为portal现有的功能模块，未来，可以将API-plane工程作为API-Server的职责模块迁移至portal工程中，这也是合理的。<br><img src="/../../img/funcDesign/%E7%BB%84%E4%BB%B6%E4%BB%A3%E7%A0%81%E5%90%88%E5%B9%B6.png" alt="组件代码合并.png"></p><p>这样将两个相互调用的微服务合并为同一个工程的两个 model，会带来以下好处：</p><ul><li>减少通信成本：将原本需要通过网络通信的两个微服务合并到同一个工程中，可以减少网络通信的成本和延迟。</li><li>简化部署操作：将两个微服务合并为同一个工程，可以简化部署操作，减少部署时间和错误。</li><li>提高程序的可维护性：将两个微服务合并为同一个工程，可以减少代码的重复，降低维护成本。</li><li>api-server与DB之间可以通过事务保证资源数据的一致性</li></ul><p>在考虑将两个微服务合并为同一个工程时，api-plane所有的controller都将变成普通方法由portal进行调用，存在相当大的改造以及测试成本。<br><img src="/../../img/funcDesign/%E5%89%8D%E7%AB%AF%E7%AE%A1%E6%8E%A7%E5%A4%9A%E9%9B%86%E7%BE%A4.png" alt="前端管控多集群.png"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>综上为未来网关组件合并的探索，但现阶段标品如果按照以上的内容进行部署配置上的更改会给SRE带来很大的部署成本且原先的部署helm脚本会变得更复杂，因此我们前期先在开源版本进行组件的合并，后续在完成包括服务模块合并到portal服务中之后，我们再将helm脚本迁移到标品的版本。</p>]]></content>
      
      
      <categories>
          
          <category> Hango </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SCG </tag>
            
            <tag> 云原生网关 </tag>
            
            <tag> Hango </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>探索分布式事务解决方案以及实现原理</title>
      <link href="/2023/01/14/funcdesign/tan-suo-fen-bu-shi-shi-wu-jie-jue-fang-an-yi-ji-shi-xian-yuan-li/"/>
      <url>/2023/01/14/funcdesign/tan-suo-fen-bu-shi-shi-wu-jie-jue-fang-an-yi-ji-shi-xian-yuan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>如今分布式系统和微服务架构的盛行，一个普通的操作可能在服务端就得由多个服务和数据库实例协同完成的。特别是在互联网金融等一致性要求较高的场景下，多个独立操作之间的一致性问题显得格外棘手。随着业务的快速发展、业务复杂度越来越高，几乎每个公司的系统都会从单体走向分布式，特别是转向微服务架构，随之而来就必然遇到分布式事务这个难题，而分布式事务管理服务正是为了解决这样的问题而诞生。<br>本篇文章也将分享常见的几个分布式事务的解决方案，也是过去一段时间我在分布式事务学习的一些沉淀。</p><h2 id="什么是分布式事务"><a href="#什么是分布式事务" class="headerlink" title="什么是分布式事务"></a>什么是分布式事务</h2><p>说到事务大家了解，所有的事务都必须要满足ACID的原则，也就是原子性，一致性，隔离性，持久性。<br><img src="/../../img/%E4%BA%8B%E5%8A%A1acid.png" alt="事务acid.png"></p><p>在以前的单体架构中，往往只有一个服务，这个服务只访问一个数据库，业务比较简单。基于数据库本身的特性就已经能实现ACID了。但是现在我们要研究的微服务，微服务的业务往往比较复杂，一个业务就会跨越多个服务，每个服务都会有自己的数据库或者数据源，这时候如果还靠业务自己的数据库是难以实现整个业务的ACID的。<br><img src="/../img/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%97%AE%E9%A2%98.png" alt="分布式事务的问题.png"></p><p>在分布式系统下，一个业务跨越多个服务或数据源，每个服务都有一个本地事务，要保证所有本地事务最终状态一致，这样的事务就是分布式事务。</p><h2 id="分布式事务理论基础"><a href="#分布式事务理论基础" class="headerlink" title="分布式事务理论基础"></a>分布式事务理论基础</h2><p>解决分布式事务的理论基础主要是两个理论，一个是CAP定理，一个是Base理论</p><ul><li>CAP定理：</li></ul><p>2000年，加州大学的计算机科学家 Eric Brewer提出，分布式系统有三个指标，Consistency（一致性），Availability（可用性），Partition     tolerance（分区容错性）。但我们都知道在分布式系统中，分区是没有办法避免的，而在发生网络分区的时候，强一致性和可用性只能二选一，这就是CAP定理。</p><p><img src="/../img/cap.png" alt="cap.png"></p><ul><li>Base理论</li></ul><p>而在实际的场景下，一致性和可用性都非常地重要，两者都不想放弃，而BASE理论则是可以解决这个问题。</p><p>BASE理论是对CAP的一种解决思路，包含三个思想：</p><ul><li>Basically Available（基本可用）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。</li><li>Soft State（软状态）：在一定时间内，允许出现中间状态，比如临时的不一致状态。</li><li>Eventually Consistent（最终一致性）：虽然无法保证强一致性，但在软状态结束后，最终达到一致性</li></ul><p>其实BASE理论就是对CAP理论中的C和A的矛盾所做的调和和选择，在CAP理论中我们讲想要达到一致性，那么就要牺牲可用性，但是在BASE里我们说，如果想要达成强一致性，是要牺牲可用性，但不是完全不可用，而是允许损失部分可用性，即保证核心可用。<br>就比如说在ES集群中，有一个节点发生了故障，我们会将其从集群中剔除，这时候当前节点不可用，但是没关系，一旦它的网络恢复了，我们会重讲将它加入集群之中，重新给它分片，使其重新可用。反过来，如果我们想要达到完整的可用性，在CAP中需要牺牲一致性，但是在BASE理论中，只是临时的不一致，在软状态结束后，我们会使其最终达到一致性。</p><p>而在我们分布式事务中，往往包含多个子事务，各个子事务各自执行和提交，结果有些成功，有些失败，这时候大家的状态不一致，但是我们希望事务中的各个子事务状态能够一致，要么大家都成功，要么大家都失败。因此我们可以借鉴CAP定理和BASE理论解决分布式最大的问题是各个子事务的一致性问题。</p><p>第一种 解决方案就是基于AP的模式，各个子事务分别执行和提交，允许出现结果不一致，然后采取弥补措施恢复数据即可，实现最终一致。</p><p>第二种是基于CP模式，各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务执行过程中，处于弱可用状态。</p><h2 id="分布式事务模型"><a href="#分布式事务模型" class="headerlink" title="分布式事务模型"></a>分布式事务模型</h2><p>但是不论基于AP还是CP模式，这里都有一个共同点，那就是各个本地事务都需要互相通信，来辨别彼此的通信，但是各个子事务之间怎么进行通信呢？所以它就是需要有一个协调者来帮助分布式事务中的各个子事务进行通信，感知彼次的状态。<br><img src="/../img/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%9E%8B.png" alt="分布式事务模型.png"></p><p>这里的子事务系统称作分支事务，有关联的各个分支事务组合在一起称为全局事务</p><p>在分布式事务管理系统中由三个重要的角色组成：</p><ul><li>TC（Transaction Coordinator）-事务协调者：维护全局和分支事务的状态，协调全局事务提交或回滚</li><li>TM（Transaction Manager）-事务管理器：定义全局事务的范围，开始全局事务，提交或回滚全局事务</li><li>RM（Resource Manager）-资源管理器：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚</li></ul><p><img src="/../img/tctmrm.png" alt="tctmrm.png"></p><p>一个典型的事务过程包括：</p><ul><li>TM 向 TC 申请开启（Begin）一个全局事务，全局事务创建成功并生成一个全局唯一的 XID。</li><li>XID 在微服务调用链路的上下文中传播。</li><li>RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖。</li><li>TM 向 TC 发起针对 XID 的全局提交（Commit）或回滚（Rollback）决议。</li><li>TC 调度 XID 下管辖的全部分支事务完成提交（Commit）或回滚（Rollback）请求。</li></ul><p>当然这只是GTXS中的事务模型，并不是最终的解决方案。在实际的分布式事务管理系统中会根据业务是要做强一致还是最终一致延伸出了好几种解决方案。比如有XA模式，AT模式，TCC模式，SAGA模式。</p><h2 id="分布式事务常见解决方案"><a href="#分布式事务常见解决方案" class="headerlink" title="分布式事务常见解决方案"></a>分布式事务常见解决方案</h2><h3 id="XA模式"><a href="#XA模式" class="headerlink" title="XA模式"></a>XA模式</h3><p>XA规范是X/Open 组织定义的分布式事务处理的标准，那么这个标准叫DTP标准，可以认为是分布式事务领域最早的一个标准。所以几乎所有主流的数据库都对XA规范提供了支持。比如MYSQL ，ORACLE都实现了这种标准，换言之这些数据库内部都已经实现了分布式事务，而采用的模式则是XA模式。这种分布式事务把XA模式定义成了两个阶段。</p><p>第一阶段叫做准备阶段，事务协调者在准备阶段会向资源管理者RM发起准备的请求，在以下图片中RM是由事务参与者的SDK组成的，但是在这里RM是由数据库本身实现的，所以在此事务协调者会通知各个微服务的数据库执行自己的sql，但是执行完不要提交，而是把执行执行的结果告诉事务调节者进行就绪，事务协调者会根据各个事务的执行情况判断下一阶段的事情。</p><p>如果都成功了则通知各个分支事务的数据库进行提交，否则让他们进行事务的回滚。所以我们看到这种XA模式就是基于数据库本身的特性来实现的分布式事务，因为一阶段并不提交，在二阶段才进行提交或回滚，它们是能够满足事务的ACID的特性的，它是一种强一致性的事务。</p><p><img src="/../img/XA%E6%A8%A1%E5%BC%8F.png" alt="XA模式.png"></p><h4 id="XA模式的优势与不足"><a href="#XA模式的优势与不足" class="headerlink" title="XA模式的优势与不足"></a>XA模式的优势与不足</h4><ul><li>优点</li></ul><p>前面我们提到各个分支事务在一阶段的时候只是执行事务而不提交，那么这个事务一直处于运行中的一个状态，那么我们知道事务本身就具备ACID的特性，到了二阶段时候，所有的事务都执行完了再一起提交，所以每个分支事务都具备ACID的能力，而每个事务之间都互相等待，所以整个全局事务都具备ACID的特性。所以这种XA模式具备强一致性的，这是它的第一个优势。那么第二个优势就是说数据库本身就已经实现了分布式事务的能力，我们只是在其基础上做了封装，实现起来也比较简单，在使用的时候是没有代码入侵的，这是它第二个优势</p><ul><li>缺点</li></ul><p>由于分支事务在执行完业务sql的时候并不会进行事务的提交，而是等待TC的过程中会占用数据库锁，如果其他分支事务耗时较长，整个过程中所有的分支事务都处于等待的状态，其他事务都没办法进行资源的访问，造成了资源的浪费，所以它的性能是非常差的，可用性就降低了。第二个缺点，XA模式是依赖于数据库的实现，如果数据库不支持XA的规范，就比如说Redis这种非关系型的数据库，XA模式就没办法实现，这是它的第二个缺点。</p><h3 id="AT模式"><a href="#AT模式" class="headerlink" title="AT模式"></a>AT模式</h3><p>AT模式同样是分阶段提交的事务模型，不过弥补了XA模型中资源锁定周期过长的缺陷。整体来说它还是这个模型，一开始都是一样的，TM开启全局事务并完成全局事务的注册，然后去调用我们的每个分支，每个RM都到TC进行分支事务的注册，并且执行本地业务SQL，但是此处AT执行完SQl之后会立即提交分支事务，而不是等待其他事务的提交，所以它的性能会优于XA模式。但是为了能够进行分支事务的回滚，RM会拦截SQL的执行并且给数据形成快照undo log，这时一阶段就可以放心大胆地提交，然后将状态报告给TC。第二阶段TM在执行完分支事务的调用之后会通知TC进行全局事务的提交。TC会根据分支事务的状态来判断提交或回滚。此处的提交并不是数据库事务的提交，而是通知各个分支事务RM将undo log 进行删除，并且是异步删除，进一步提高性能。如果存在失败的分支事务，则会通知后各个分支事务进行数据的回滚，一旦回滚成功，也需要将undo log进行删除。这就是二阶段的变化。以上就是AT模式的玩法。</p><p><img src="/../img/fmt%E6%A8%A1%E5%BC%8F.png" alt="fmt模式.png"></p><h4 id="AT模式的优势与不足"><a href="#AT模式的优势与不足" class="headerlink" title="AT模式的优势与不足"></a>AT模式的优势与不足</h4><ul><li><p>优点</p><p>AT是基于XA 2PC的模式实现的，针对于木桶效应，通过业务数据提交时自动拦截所有 SQL，将 SQL 对数据修改前、修改后的结果分别保存快照，生成行锁，通过本地事务一起提交到操作的数据源中，相当于自动记录了重做和回滚日志。这样每一个数据源都可以单独提交，然后立刻释放锁和资源。</p></li><li><p>缺点</p><p>提高了吞吐量的同时，破坏了隔离性，会出现脏写的情况（在回滚前数据发生了改变，导致无法逆向SQL来补偿数据）</p></li></ul><h3 id="TCC模式"><a href="#TCC模式" class="headerlink" title="TCC模式"></a>TCC模式</h3><p>TCC模式和AT模式相似，每个阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复，需要实现三个方法：<br>Try：资源的检测和预留<br>Confirm：完成资源操作业务，要求Try成功 Confirm一定要能成功<br>Cancel：预留资源释放，可以理解Try的反向操作</p><p><img src="/../img/TCC%E6%A8%A1%E5%BC%8F.png" alt="TCC模式.png"></p><h3 id="SAGA模式"><a href="#SAGA模式" class="headerlink" title="SAGA模式"></a>SAGA模式</h3><p>SAGA模式是GTXS提供的长事务解决方案，也分为两个阶段。它在一阶段的时候直接提交本地事务，而在二阶段的过程中如果成功则什么都不做，如果失败则通过编写补偿业务来回滚。聊到这大家可能觉得这和TCC模式也很像，但其实不一样。因为TCC在一阶段并不是提交事务，而是做资源的预留而已，这里一阶段直接提交事务。<br>而第二阶段也不一样，TCC在第二阶段是对预留资源的扣除或者回滚，而SAGA在第二阶段如果成功则什么事情都不做，如果失败则通过编写补偿方法来回滚事务。<br>但也因为SAGA模式在第二阶段直接操作的资源本身，所有它也失去了TCC模式中资源的隔离效果，所以SAGA模式中最大的缺点就是没有隔离性，事务与事务之间是可能存在脏写的，所以它是有隔离的安全问题。它的执行流程有下面这张图。它会在第一阶段按照顺序逐个执行分支事务，而一旦在这个事务流程有事务的执行出现了问题，则会反向地去执行补偿逻辑，从而保证整个事务地状态一致性。<br>它还有一个特点是，它这里地每个分支事务是可以基于事件驱动的，而这种事件有一个好处就是它的吞吐能力会比较强，它不会阻塞和等待，你们可以理解它是一种状态机的机制来实现。但也因此每个分支事务的执行事件不确定，所以它的时效性可能比较差。<br>这种模型一般比较适用于事务跨度比较大的情况的场景来使用，但是一般SAGA的使用场景是比较少的，一般都是TCC或者FMT模式。</p><p><img src="/../img/SAGA%E6%A8%A1%E5%BC%8F.png" alt="SAGA模式.png"></p><h4 id="SAGA模式的优势与不足"><a href="#SAGA模式的优势与不足" class="headerlink" title="SAGA模式的优势与不足"></a>SAGA模式的优势与不足</h4><ul><li>优点</li></ul><p>事务参与者可以基于事件驱动实现异步调用，吞吐高</p><p>一阶段直接提交事务，无锁，性能好</p><p>不用编写TCC中的三个阶段，实现简单</p><ul><li>缺点：</li></ul><p>软状态持续时间不确定，时效性差</p><p>没有锁，没有事务隔离，会有脏写</p><h3 id="四种模式对比"><a href="#四种模式对比" class="headerlink" title="四种模式对比"></a>四种模式对比</h3><p><img src="/../img/%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94.png" alt="四种模式对比.png"></p>]]></content>
      
      
      <categories>
          
          <category> 分布式事务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式事务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloudGateway功能原理以及产品化实践</title>
      <link href="/2022/06/11/funcdesign/springcloudgateway-gong-neng-yuan-li-yi-ji-chan-pin-shi-jian/"/>
      <url>/2022/06/11/funcdesign/springcloudgateway-gong-neng-yuan-li-yi-ji-chan-pin-shi-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>近日，我司的项目总监让我们网关组的成员对企业级SpringCloudGateway网关相关内容进行整理，我正好借此机会沉淀一下这段时间以来对网关的开发实践以及思考。接下来我会按照以下内容进行介绍：</p><ul><li>介绍企业级微服务开发面临的痛点、需求、场景和应用</li><li>介绍SpringCloud Gateway的核心原理</li><li>介绍SpringCloud Gateway在网关高可用、动态路由、可扩展插件、接口调试、限流熔断、性能优化等方面的实践与优化</li><li>介绍自己对企业级微服务网关的一些思考与未来的发展方向</li></ul><h2 id="微服务开发面临的问题"><a href="#微服务开发面临的问题" class="headerlink" title="微服务开发面临的问题"></a>微服务开发面临的问题</h2><ul><li>服务需要重复开发通用功能，随着微服务规模的不断扩大，这些校验冗余逻辑将越来越沉重，一旦校验规则有了变化，不得不去每个应用修改这些逻辑，增加了维护成本</li><li>随着时间的推移，可能需要改变系统目前的拆分方案，但如果客户端直接与微服务交互，强耦合，那么这种重构就很难实施</li><li>服务协议不统一，系统服务使用webService、gRPC以及其他RPC等非RESTFUL接口标准协议进行开发的应用，协议不统一，需要兼容</li><li>业务不断发展需求快速迭代，服务接口愈多，如何保护、监控、维护数以万计的接口</li></ul><h2 id="微服务网关的应用场景有哪些"><a href="#微服务网关的应用场景有哪些" class="headerlink" title="微服务网关的应用场景有哪些"></a>微服务网关的应用场景有哪些</h2><ul><li>微服务网关</li><li>业务系统集成</li><li>企业能力开放</li><li>接口生命周期管理</li><li>架构治理</li></ul><h2 id="Spring-Cloud-Gateway的核心原理"><a href="#Spring-Cloud-Gateway的核心原理" class="headerlink" title="Spring Cloud Gateway的核心原理"></a>Spring Cloud Gateway的核心原理</h2><p>Spring Cloud Gateway是Spring官方基于Spring5.0、SpringBoot2.0和Project Reactor等技术开发的网关旨在为微服务框架提供一种简单而有效的 统一的API路由管理方式，统一访问接口。Spring Cloud Gateway作为Spring Cloud生态体系中的网关，目标是替代Netflix的Zuul，其不仅提供统一的路由方式，并且<br>基于Filter链的方式提供了网关基本的功能，例如：安全、监控/埋点和限流等等。</p><p>Spring Cloud Gateway网关是一个内外衔接的数据交换组件，对内API接口的方式纳管所有要对外透出的微服务，作为出口端点，对外提供API接口给上游的Web应用、Mobile应用、外部微服务。网关核心在于将请求流量由上游发起经过网关到下游的微服务，在流量出入的过程中，网关在路由策略，协议转换、过滤、API组合等方面构建<br>网关的核心能力。</p><p><img src="/../../img/scg%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86.png" alt="scg核心原理.png"></p><h3 id="关键术语"><a href="#关键术语" class="headerlink" title="关键术语"></a>关键术语</h3><ul><li>路由Route：即一套路由规则，是集URI、predicate、filter等属性的一个元数据类。</li><li>断言Predicate：Java8函数断言，这里可以看做是满足什么条件的时候，route规则进行生效。允许开发者去定义匹配来自于Http Request中的任何信息，如请求头和参数。</li><li>过滤器Filter：filter针对请求和响应进行增强、修改处理。filter可以认为是Spring Cloud Gateway最核心的模块，熔断、安全、逻辑执行、网络调用都是filter来完成的，其中又细分为gateway filter和global filter，区别在于是具体一个route规则生效还是所有route规则都生效。</li></ul><p><img src="/../../img/scg%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%862.png" alt="scg核心原理2.png"></p><h2 id="CSB2-0云原生网关如何保障服务高可用"><a href="#CSB2-0云原生网关如何保障服务高可用" class="headerlink" title="CSB2.0云原生网关如何保障服务高可用"></a>CSB2.0云原生网关如何保障服务高可用</h2><p>项目术语：</p><ul><li>console: 网关配置控制台的云原生网关服务</li><li>broker: 实际请求转发的云原生网关服务</li><li>SLA:service-level agreement SLA的概念，对互联网公司来说就是服务可用性的一个保证。</li><li>SLB:Server Load Balancing 指服务器负载均衡</li></ul><p>网关承载着所有服务的入口流量,要求具备高可用的能力。而通常实现高可用的主要手段是数据的冗余备份和服务 的失效转移,而这两种手段在网关的具备体现为：</p><h3 id="集群的部署结构："><a href="#集群的部署结构：" class="headerlink" title="集群的部署结构："></a>集群的部署结构：</h3><p>CSB2.0云原生网关目前是基于Kubernetes容器编排平台进行部署，由云平台提供SLB作为流量到网关节点的负载均衡能力，网关根据服务器的性能进行集群的部署，这部分的SLA的能力由云平台负责保障。企业部署一个地方的网关节点集群，相当于单数据中心，也可根 据自己的需求部署多个网关节点集群，以达到多个数<br>据中心的容灾能力，这样部署就已经能保障网关的正常可用。如图所示：<br><img src="/../../img/scg%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.png" alt="scg集群部署.png"></p><h3 id="负载均衡的能力："><a href="#负载均衡的能力：" class="headerlink" title="负载均衡的能力："></a>负载均衡的能力：</h3><p>CSB2.0云原生网关实现了一套控制台即可对不同实例下的网关节点进行配置下发，并且同一个实例内的所有网关节点都是生效的。通常一个网关节点部署在一个服务器上，节点通过监听中间件(Redis、Nacos等)中的配置信息以此来获取、更新broker中的配置。此处的负载均衡是指broker节点对于下游服务的不同节点进行<br>请求转发，例如服务service1的请求会依次分发到服务下的节点node1、node2 和node3。如图所示：<br><img src="/../../img/scg-lb.png" alt="scg-lb.png"></p><h3 id="健康检查"><a href="#健康检查" class="headerlink" title="健康检查"></a>健康检查</h3><p>请求到达服务节点前，尽管已经做了两层的负载，即请求到达云容器平台的负载均衡器以及broker对下游服务节点的负载。但是不管是网关broker节点，还是下游的某个节点发生了故障,请求还是有可能继续打到该节点上，这时候服务的最终结果仍是不可用。解决的方案是把所有有问题的的节点，包括broker节点、下游服务节点移出SLB的范围即可。因此需要解决的问题是如何知道broker节点以及下游服务的node节点是否正常？解决方案是对broker节点以及服务下游node节点进行健康检查：</p><ul><li>broker节点健康检查：CSB2.0云原生网关的部署依托于Kubernetes等容器编排平台，容器平台提供有多种对容器的健康检查方式，如容器探针、基于HTTP的探活检查</li><li>下游服务健康检查：可对下游服务设置正确的返回结果(请求状态码、超时期限等),定时轮训将请求打到下游服务，若发现返回异常且满足从负载节点移除的条件，则将该节点移除。当下游服务节点恢复再加回负载列表。</li></ul><h3 id="节点恢复"><a href="#节点恢复" class="headerlink" title="节点恢复"></a>节点恢复</h3><p>CSB2.0云原生网关依托于Kubernetes云容器编排平台进行部署，节点的自恢复能力由云平台保证。</p><h3 id="熔断与降级"><a href="#熔断与降级" class="headerlink" title="熔断与降级"></a>熔断与降级</h3><p>下游服务可能会出现一些超出预期的错误，这种错误有可能影响到系统的正常运行，比如请求引发的阻塞，这种请求阻塞有可能占用系统宝贵的资源，如：内存、线程、数据库等，消耗的资源有可能会拖垮整个系统，因此网关<br>需要判断服务不可用就切断对服务的访问,CSB2.0云原生网关底层采用阿里开源的sentinel框架作为流量控制、熔断降级、系统负载保护等多个纬度保障服务的稳定性。<br>当网关请求在一段时间内失败次数达到一定条件，就会触发熔断。目前CSB2.0支持的熔断条件有：<br>1、后端响应时间<br>2、后端错误码<br>3、触发熔断的请求阈值<br>降级策略目前支持Mock的形式返回响应参数，包括响应码、响应头、响应体。<br>当发生熔断后，判断请求是否恢复正常的条件，若连续请求成功次数达标，则恢复转发，服务自动转入监控期；否则，继续进入熔断期。如此反复。如下图：<br><img src="/../../img/%E7%86%94%E6%96%AD.png" alt="熔断.png"></p><h3 id="接口重试"><a href="#接口重试" class="headerlink" title="接口重试"></a>接口重试</h3><p>虽然有很多机制保障接口的可访问，但是一个请求报错的原因有很多，偶然一次报错不一定是服务不可用，最简单的，第一次不行，应该再访问一次或几次，以确定结果。 请求重试可以说是网关对接口转发的基本要求，每个接口都应该可以设置重试次数。当请求失败后，网关应立即再次请求，直到拿到正常返回，或是达到重试阈值，再将结果返回给客户端。</p><h2 id="CSB2-0云原生网关如何实现动态路由"><a href="#CSB2-0云原生网关如何实现动态路由" class="headerlink" title="CSB2.0云原生网关如何实现动态路由"></a>CSB2.0云原生网关如何实现动态路由</h2><p>为了应对网关路由各种复杂业务场景，要求网关在不重启服务的情况下，实现对API路由规则的动态配置，实时生效。SpringCloud提供有两种原生动态路由的方式：</p><ul><li>Spring Cloud DiscoveryClient原生支持：<br>Spring Cloud原生支持服务自动发现并且注册到路由之中，通过在application.properties中设置spring.cloud.gateway.discovery.locator.enabled=true, 同时确保DiscoveryClient的实体(Nacos，Netflix Eureka, Consul, 或 Zookeeper) 已经生效，即可完成服务的自动发现及注册。</li><li>一种是基于Actuator API<br>SpringCloud Gateway提供有OpenAPI来建立路由信息，请求内容为JSON请求体，请求方法为POST 如路径：/gateway/routes/{id_route_to_create}</li></ul><p>以上路由扩展的自由度有限，第一种方式的服务都要依托与SpringCloud家族体系下，第二种无法满足高度定制化的需求。CSB2.0的做法是对SpringCloud Gateway做了底层修改，扩展了Spring Cloud Gateway底层路由加载机制，将Spring Cloud Gateway运行态时保存的路由关系，通过实现、继承加载自定义类的方式，对其进行动态路由修改，每当路由有变化时，再触发一次动态的修改。</p><p>因此这种实现需要两种保障：<br>1、监听机制<br>2、实现自定义路由的核心类<br>Spring Cloud Gateway 核心加载机制如图所示：</p><p><img src="/../../img/scg%E9%85%8D%E7%BD%AE%E7%9B%91%E5%90%AC.png" alt="scg配置监听.png"></p><h2 id="CSB2-0云原生网关如何实现插件热插拔"><a href="#CSB2-0云原生网关如何实现插件热插拔" class="headerlink" title="CSB2.0云原生网关如何实现插件热插拔"></a>CSB2.0云原生网关如何实现插件热插拔</h2><p>CSB2.0插件热插拔还在排期开发，但可参考 <a href="https://juejin.cn/post/6963453967497953311">https://juejin.cn/post/6963453967497953311</a></p><h2 id="CSB2-0云原生网关如何实现接口调试"><a href="#CSB2-0云原生网关如何实现接口调试" class="headerlink" title="CSB2.0云原生网关如何实现接口调试"></a>CSB2.0云原生网关如何实现接口调试</h2><p>CSB2.0需要Console对Broker进行接口调试，但在专有云网络下存在【管控区】和【用户区】的区分，两者之间存在网络通信限制，具体表现为</p><ul><li>用户区访问管控区，可以通过 vip 打通</li><li>管控区不允许访问用户区</li></ul><p>这里的“访问”在 TCP 层面可以理解为不允许主动建立连接，而 API 调试的需求从功能层面来看，的确是【管控区】访问【用户区】，在无法主动建立连接的情况下，需要设计网络反向访问方案。</p><p>本方案提出一个设计，由【用户区】的 broker 主动建立 TCP 长连接到【管控区】的 console，借助于 TCP 双工通信的特性，console 可以通过持续维护 broker 的连接，从而完成与 broker 的通信。<br>整体设计如图所示：<br><img src="/../../img/%E6%8E%A5%E5%8F%A3%E8%B0%83%E8%AF%95%E6%95%B4%E4%BD%93%E6%96%B9%E6%A1%88.png" alt="接口调试整体方案.png"></p><h3 id="VIP连接方案"><a href="#VIP连接方案" class="headerlink" title="VIP连接方案"></a>VIP连接方案</h3><p>长连接实现方案使用 WebSocket实现 console 与 broker 的交互。但是在实际部署中，【用户区】的 broker 经过了一个 VIP 连接到【管控区】的 console，由于 VIP 随机负载均衡的特性的，可能会出现部分 console 没有持有长连接的问题，当这些 console 节点接受请求之后，将会无法处理。如图所示：</p><p><img src="/../../img/scg-vip%E9%97%AE%E9%A2%98.png" alt="scg-vip问题.png"></p><p>为了解决上述问题，如下图引入心跳机制。broker 通过定时任务向 VIP 发送建连请求，console 在接受到请求之后，需要返回 console ip，broker 在接收到 console ip 的响应之后，<br>需要比对 console ip 与本地的 console ip 缓存池，从而判断是否是一条新的连接</p><ul><li>从连接层面来看，broker 与 vip 建立了重复的连接，但实际上连接到了不同的 console 实例</li><li>轮询机制保障了最终能够与所有 console 实例建立连接</li><li>轮询机制兼容了 console 动态扩缩容的场景</li><li>轮询机制兼容了连接断开的场景</li></ul><p><img src="/../../img/%E6%8E%A5%E5%8F%A3%E8%B0%83%E8%AF%95%E6%9C%80%E7%BB%88%E8%AE%BE%E8%AE%A1.png" alt="接口调试最终设计.png"></p><h2 id="CSB2-0云原生网关如何实现多协议转换"><a href="#CSB2-0云原生网关如何实现多协议转换" class="headerlink" title="CSB2.0云原生网关如何实现多协议转换"></a>CSB2.0云原生网关如何实现多协议转换</h2><p>在SpringCloud-Gateway网关中扩展多种协议如：Grpc、WebService、Dubbo、HSF等协议，在此之前我们需要分析一下SpringCloud-Gateway filter机制。springcloud-gateway基于过滤器实现， 分为pre和post两种类型的过滤器，分别处理前置逻辑和后置逻辑。客户端将http请求到gateway，请求经过前置过滤器处理后转发到具体到业务服务中，收到业务服务的相应<br>后，响应将通过后置过滤器处理后返回客户端，其中过滤器的处理顺序按照order排序（后置处理器倒序排序）;<br><img src="/../../img/scg-filter%E6%B5%81%E7%A8%8B.png" alt="scg-filter流程.png"></p><p>对于http-http的请求代理来说，NettyRoutingFilter是作为filter chain的最后一个pre filter，它负责将请求转发到具体的业务服务中</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">NettyRoutingFilter</span> <span class="token keyword">implements</span> <span class="token class-name">GlobalFilter</span><span class="token punctuation">,</span> <span class="token class-name">Ordered</span> <span class="token punctuation">{</span>      <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getOrder</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> <span class="token class-name">Ordered</span><span class="token punctuation">.</span>LOWEST_PRECEDENCE<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>NettyWriteResponseFilter作为post filter chain的第一个filter，它负责将业务服务的响应返回给客户端</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">NettyWriteResponseFilter</span> <span class="token keyword">implements</span> <span class="token class-name">GlobalFilter</span><span class="token punctuation">,</span> <span class="token class-name">Ordered</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">int</span> WRITE_RESPONSE_FILTER_ORDER <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>  <span class="token annotation punctuation">@Override</span>  <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getOrder</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> WRITE_RESPONSE_FILTER_ORDER<span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因此要实现http作为入口协议的多协议的泛化调用，我们需要在pre filter chain中添加一个filter代替NettyRoutingFilter，负责将http编码为其他协议的请求，同时在post filter chain中添加一个filter代替NettyWriteResponseFilter，负责将其他协议的响应转换为http响应。<br>以dubbo的泛化调用为例，实现如下图所示：<br><img src="/../../img/scg%E6%B3%9B%E5%8C%96%E8%B0%83%E7%94%A8.png" alt="scg泛化调用.png"></p><h2 id="对企业级微服务网关的一些思考与未来的发展方向"><a href="#对企业级微服务网关的一些思考与未来的发展方向" class="headerlink" title="对企业级微服务网关的一些思考与未来的发展方向"></a>对企业级微服务网关的一些思考与未来的发展方向</h2><p>我们可以看到Spring Cloud Gateway可以很好地与其背后的Spring 社区和 SpringCloud 微服务体系有着很好的适配和集成，这与 Java 语言流行的原因如出一辙。如果一个企业主打的技术栈是Java 体系，那么基于SpringBoot/ SpringCloud 开发微服务，选型 SpringCloud Gateway 作为微服务网关，会有着得天独厚的优势。<br>而在企业级微服务网关的探索上，我认为可以基于一下几点进行后续的演进：</p><ul><li>多协议入口网关建设：基于scg作为网关引擎更多使用场景为南北向的流量转发，但是在企业级网关建设中，往往需要支持多种协议的入口，如：dubbo、grpc以及私有协议等东西向流量的转发，因此需要在scg的基础上进行扩展，支持多种协议的入口。</li><li>性能与稳定性建设: 优化Spring Cloud Gateway的性能，确保在高并发情况下的稳定性和低延迟。</li><li>云原生支持: 加强与Kubernetes的集成，支持容器化和微服务自动化部署。</li><li>云服务厂商整合: 提供对主要云服务厂商的无缝整合，如AWS, Azure, GCP等。</li></ul>]]></content>
      
      
      <categories>
          
          <category> SpringCloudGateway </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloudGateway </tag>
            
            <tag> SCG </tag>
            
            <tag> 云原生网关 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网关可观测性建设-(SpringCloudGateway篇)</title>
      <link href="/2022/05/14/funcdesign/wang-guan-ke-guan-ce-xing-jian-she-springcloudgateway-pian-zhang/"/>
      <url>/2022/05/14/funcdesign/wang-guan-ke-guan-ce-xing-jian-she-springcloudgateway-pian-zhang/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 云原生网关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloudGateway </tag>
            
            <tag> 云原生网关 </tag>
            
            <tag> 可观测性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务网格</title>
      <link href="/2022/03/24/servicemesh/fu-wu-wang-ge/"/>
      <url>/2022/03/24/servicemesh/fu-wu-wang-ge/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是服务网格？"><a href="#什么是服务网格？" class="headerlink" title="什么是服务网格？"></a>什么是服务网格？</h3>]]></content>
      
      
      <categories>
          
          <category> ServiceMesh </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ServiceMesh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes的设计解读</title>
      <link href="/2022/03/20/kubernetes/kubernetes-de-she-ji-jie-du/"/>
      <url>/2022/03/20/kubernetes/kubernetes-de-she-ji-jie-du/</url>
      
        <content type="html"><![CDATA[<h3 id="pod-设计解读"><a href="#pod-设计解读" class="headerlink" title="pod 设计解读"></a>pod 设计解读</h3><p>在kubernetes中，创建、调度、管理的最小单位是pod</p><ul><li>pod是IP等网络资源的分配的基本单位，这个IP及其对应的network namespace是由pod里面的容器共享的</li><li>pod内的所有容器页共享volume。当一个volume被挂载在同属同一个pod的多个Docker容器的文件系统</li><li>IPC namespace 即同一个pod内的应用容器能够使用System V IPC或者POSIX消息队列进行通信</li><li>UTS namespace 即同一个pod内的应用容器共享主机名</li></ul><p>1.label 和label selector与pod协作</p><p>2.pod的现状和未来</p><ul><li>资源共享和通信</li><li>集中式管理，指pod内的所有容器资源</li></ul><p>3.pod内的容器网络与通信</p><p>​    通过pause容器进行pod内的容器网络与通信</p><ul><li><p>replication controller设计解读</p><p>replication controller在设计上依然体现出了”旁路控制”的思想，为每个pod “外挂”了一个控制器进程，从而避免了健康检查组件成为性能瓶颈</p><p>replication controller只能与重启策略为Always的pod进行协作</p><p>replication controller的经典场景:</p><ul><li>重调度</li><li>弹性伸缩</li><li>滚动更新</li><li>多版本应用release追踪</li></ul></li><li><p>service的设计解读</p><p>service通过标签label将流量负载均衡到对应label标签的pod上</p><ul><li><p>service工作原理</p><p>Kubernetes集群上的每个节点都运行着一个服务代理(service proxy),它是负责实现service的主要组件</p><p>kube proxy两种工作模式：</p><ul><li><p>userspace模式</p><p>对于每个service kube-proxy都会在宿主机监听一个端口与这个service对应起来，并在宿主机上建立起iptables规则，将service IP:service port的流量重定向到上述端口</p></li><li><p>iptables模式</p><p>iptables模式下的kube-proxy将负责创建和维护iptables的路由规则，其余工作交由内核态的iptables完成</p></li></ul></li><li><p>service的自发现机制</p><ul><li>环境变量方式</li><li>DNS方式(mysvc.myns)</li></ul></li><li><p>service 外部可路由性设计</p><ul><li>NodePort</li><li>LoadBalancer</li><li>external ip</li></ul></li></ul></li><li><p>新一代版本控制器 replica set</p><p>replica set 用于保证label selector 匹配的pod数量维持在期望状态</p><p>replicat set 与 replication controller 的区别是：replication controller只支持等值匹配   replicat set支持基于子集的查询</p></li><li><p>Deployment</p><p>Deployment多用于pod 和replica set 的更新，可以方便地跟踪观察其所属的relica set或者pod的数量以及状态变化</p></li><li><p>DaemonSet</p></li><li><p>ConfigMap</p></li><li><p>Job</p></li></ul><h3 id="Kubernetes核心-组件解读"><a href="#Kubernetes核心-组件解读" class="headerlink" title="Kubernetes核心 组件解读"></a>Kubernetes核心 组件解读</h3><h4 id="Master节点："><a href="#Master节点：" class="headerlink" title="Master节点："></a>Master节点：</h4><h5 id="APIServer"><a href="#APIServer" class="headerlink" title="APIServer:"></a>APIServer:</h5><p>Kubernetes APIserver负责对外提供Kubernetes API服务，它运行在Kubernetes的管理节点-Master节点</p><ul><li><p>APIServer的职能:</p><ul><li>对外提供RESTful的管理接口</li><li>配置Kubernetes的资源对象</li><li>提供可定制的功能性插件</li></ul></li><li><p>APIServer启动过程:</p><p>1.新建APIServer 定义一个APIServer所需的关键资源</p><p>2.接受用户命令行输入，为上述各参数赋值</p><p>3.解析并格式化用户传入的参数</p><p>4.初始化log配置</p><p>5.启动运行一个全新的APIServer</p></li><li><p>APIServer对etcd的封装：</p><p>Kubernetes使用etcd作为后台存储解决方案，而APIServer则基于etcd实现了一套RESTful API，用于操作存储在etcd中的Kubernetes对象实例</p></li><li><p>APIServer如何保证API操作的原子性:</p><p>Kubernetes的资源对象都设置了resourceVersion作为其元数据的一部分，APIServer以此保证资源对象操作的原子性</p></li></ul><h5 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler:"></a>Scheduler:</h5><p>根据特定的调度算法将pod调度到指定的工作节点上，这一过程称作绑定</p><ul><li><p>Scheduler的数据采集模型</p><p>Scheduler定时向APIServer获取各种各样它需要的数据，为了减少轮询时APIServer带来的额外开销，对于感兴趣的资源设置了本地缓存机制</p></li><li><p>Scheduler调度算法</p><p>Kubernetes中的调度策略分为两个阶段：Predicates , Priorites</p><ul><li>Predicates :回答能不能</li><li>Priorites：在Predicates基础上回答匹配度</li></ul></li><li><p>controller manager</p><p>kubernetes controller manager 运行在集群的master节点上，管理着集群中的各种控制器</p></li></ul><h4 id="工作节点："><a href="#工作节点：" class="headerlink" title="工作节点："></a>工作节点：</h4><h5 id="cAdvisor"><a href="#cAdvisor" class="headerlink" title="cAdvisor:"></a>cAdvisor:</h5><p>获取当前工作节点的宿主机信息</p><h5 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet :"></a>kubelet :</h5><p>kubelet组件 是Kubenetes集群工作节点上最重要的组件进程，它负责管理和维护在这台主机上运行着的所有容器。本质上它的工作可以归纳为使得pod的运行状态（status）与它的期待值（spec）一致。</p><p>Kubelet如何同步工作节点状态：</p><p>1.kubelet调用APIServer API向etcd获取包含当前工作节点状态信息的node 对象，查询的键值就是kubelet所在工作节点的主机名，</p><p>2.调用cAdvisor客户端API获取当前工作节点的宿主机信息，更新前面步骤获取到的node对象</p><p>3.kubelet再次调用APIServer API将上述更新持久化到etcd中</p><h5 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy :"></a>kube-proxy :</h5><p>Kubernetes基于service、endpoint等概念为用户提供了一种服务发现和反向代理服务，而kube-proxy正是这种服务的底层实现机制。</p><p>服务发现实现：</p><p>Kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个从service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。</p><p>kube-proxy主要有两种工作模式: userspace 和 iptables</p><p>userspace模式：</p><p>iptables模式：</p><p>iptables模式下的proxier值负责在发现变更时更新iptables规则，而不再为每个service打开一个本地端口，所有流量转发到pod的工作将交由内核态的iptables完成。</p><h3 id="核心组件协作流程："><a href="#核心组件协作流程：" class="headerlink" title="核心组件协作流程："></a>核心组件协作流程：</h3><h4 id="创建pod"><a href="#创建pod" class="headerlink" title="创建pod"></a>创建pod</h4><p>当客户端发起一个创建pod的请求后，kubectl向APIServer的/pods端点发送一个HTTP POST请求，请求的内容即客户端提供的pod资源配置文件</p><p>APIServer收到该REST API请求后会进行一系列的验证操作，包括用户认证，授权和资源配额控制等。验证通过后，APIServer调用etcd的存储接口在后台数据库中创建一个pod对象。</p><p>Scheduler使用APIServer 的API  定期从etcd获取或者监控系统中可用的工作节点列表和待调度pod , 并使用调度策略为pod选择一个运行的工作节点，这个过程叫绑定</p><p>绑定完成后，scheduler会调用APIServer的API在etcd中创建bingding对象，描述在一个工作节点上绑定运行的所有pod信息。同时kubelet会监听APIServer上pod的更新，如果发现有pod更新信息，则会自动在podWorker的同步周期中更新对应的pod</p><p>这正是Kubernetes实现中 “一切皆资源”的体现，即所有实体对象，消息都是作为etcd里保存起来的一种资源对待，其他所有组件间协作都通过基于APIServer的数据交换，组件间一种松耦合的状态。</p><h4 id="创建service"><a href="#创建service" class="headerlink" title="创建service"></a>创建service</h4><p>当客户端发起一个创建service的请求后，kubectl向APIServer的/service端点发送一个HTTP POST请求，请求的内容即客户端提供的service资源配置文件。</p><p>同样，APIServer收到该REST API请求后会进行一系列的验证操作。验证通过后，APIServer调用etcd的存储接口在后台数据库中创建一个service对象</p><p>kube-proxy会定期调用APIServer的API获取期望service对象列表，然后再遍历期望service对象列表。对每个service调用APIServer的API获取对应的pod集的信息，并从pod信息列表中提取pod IP和容器端口号封装成endpoint对象，然后调用APIServer的API在etcd中创建对象</p><p>在```userspace kube-proxy模式``下：</p><p>对于每个新建的service，kube-proxy会为其在本地随机分配一个随机端口号，并相应地创建一个ProxySocket，随后使用iptables工具在宿主机上建立一条从ServiceProxy到ProxySocket的了链路。同时，kube-proxy后台启动一个协程监听ProxySocket上的数据并根据endpoint实例的信息将来自客户端的请求转发给相应的service后端pod.</p><p>在<code>iptables kube-proxy模式</code>下：</p><p>对于每个新建的service，kube-proxy会为其创建对应的iptables。来自客户端的请求将由内核态iptables负责转发给service后端pod完成</p><p>最后，kube-proxy会定期调用APIService的API获取期望service和endpoint列表并与本地的service 和endpoint实例同步。</p><h3 id="Kubernetes-网络核心原理"><a href="#Kubernetes-网络核心原理" class="headerlink" title="Kubernetes 网络核心原理"></a>Kubernetes 网络核心原理</h3><h4 id="单pod单IP模型"><a href="#单pod单IP模型" class="headerlink" title="单pod单IP模型"></a>单pod单IP模型</h4><p>Kubernetes为每一个pod分配一个私有网络地址段的IP地址，通过该IP地址，pod能够跨网络与其他物理机，虚拟机或容器进行通信，pod内的容器全部共享这个pod的容器配置，彼此之间使用localhost通信。</p><h4 id="单pod单IP实现原理"><a href="#单pod单IP实现原理" class="headerlink" title="单pod单IP实现原理"></a>单pod单IP实现原理</h4><p>在每一个pod中有一个网络容器，该容器先于pod内所有用户容器被创建，并且拥有该pod的网络namespace，pod的其他用户容器使用Docker的–net=container:<id>选项加入该网络的namespace，这样就实现了pod内所有容器对于网络栈的共享</id></p><h3 id="Kubernetes-高级实践"><a href="#Kubernetes-高级实践" class="headerlink" title="Kubernetes 高级实践"></a>Kubernetes 高级实践</h3><p>应用健康检查:</p><ul><li><p>进程级健康检查</p></li><li><p>业务级健康检查:</p><p>活性探针：</p><ul><li>HTTP Get: kubelet 将调用容器内Web应用的web hook 如果返回的状态为200 和 399 之间则成功</li><li>Container Exec: kubelet 将在用户容器内执行一次命令，返回码为0 则正常</li><li>TCP Socket : 尝试建立socker,但目前尚未支持</li></ul><p>如果readiness probe的健康检查结果是fail kubelet并不会杀死容器进程，而只是将该容器所属的pod 从endpoint列表删除</p></li></ul><h3 id="Kubernetes-未来动向"><a href="#Kubernetes-未来动向" class="headerlink" title="Kubernetes 未来动向"></a>Kubernetes 未来动向</h3><p>坚持走更加开放的道路</p><p>汲取Borg与Omega的优秀设计思想</p><p>致力于树立行业标准</p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka核心技术</title>
      <link href="/2022/03/20/kafka/kafka-he-xin-ji-zhu/"/>
      <url>/2022/03/20/kafka/kafka-he-xin-ji-zhu/</url>
      
        <content type="html"><![CDATA[<h3 id="生产者消息分区机制原理剖析"><a href="#生产者消息分区机制原理剖析" class="headerlink" title="生产者消息分区机制原理剖析"></a>生产者消息分区机制原理剖析</h3><p>Kafka的三级结构：主题 - 分区 - 消息</p><ul><li>为什么要分区？</li></ul><p>分区的作用是提供负载均衡的能力，不同的分区分布在不同的机器节点上，数据的读写都是针对分区的粒度进行。可通过增加机器来增加吞吐量</p><ul><li>都有哪些分区策略</li></ul><p>1、轮训策略</p><p>2、随机策略</p><p>3、消息键保序策略</p><p>分区是实现负载均衡以及高吞吐量的关键，故在生产者这一端就要仔细盘算合适的分区策略，避免造成数据倾斜，使得某些分区成为性能瓶颈。</p><h3 id="Kafka副本机制详解"><a href="#Kafka副本机制详解" class="headerlink" title="Kafka副本机制详解"></a>Kafka副本机制详解</h3><p>Kafka副本机制的好处：</p><ul><li>提供数据冗余</li></ul><p>同一个分区下的所有副本保存有相同的消息序列，这些副本分散保存在不同的Broker上，从而能对抗部分Broker宕机带来的数据不可用</p><p>数据同步机制：</p><p>同个分区下的不同副本基于领导者的副本机制进行数据同步,从副本只负责同步数据，不负责对外的读写工作。</p><p>原因：</p><ul><li>方便实现“Read-your-writes”</li><li>方便实现单调读（Monotonic Reads）</li></ul><p>主从分区实现数据同步的保证：<em><strong>In-sync Replicas</strong></em>机制</p><ul><li>Broker 端参数 <em><strong>replica.lag.time.max.ms</strong></em> 参数值设置的是主从同步的最长间隔</li></ul><h3 id="Kafka为什么那么快"><a href="#Kafka为什么那么快" class="headerlink" title="Kafka为什么那么快"></a>Kafka为什么那么快</h3><ul><li>Kafka具有优秀的磁盘读写能力</li><li>批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。</li><li>请求采用多路复用的IO模型</li></ul><h3 id="Kafka请求是怎么处理"><a href="#Kafka请求是怎么处理" class="headerlink" title="Kafka请求是怎么处理"></a>Kafka请求是怎么处理</h3><p>Kafka在TCP的基础上封装了一组请求协议，PRODUCR 请求用于生产消息，FETCH请求用于消费消息，METADATA请求是用于请求Kafka元数据</p><p>Kafka使用的是<em><strong>Reactor</strong></em>模式处理请求。</p><p>Reactor模式是事件驱动架构的一种实现方式，特别适合用于处理多个客户端并发向服务端发送请求的场景。</p><p>client   —–&gt;  Reactor(Dispatcher) 公平分配  ——-&gt;read(网络线程池   ——&gt;共享请求队列   ——&gt;IO线程池  )   ——-&gt;decode  —–&gt; 网络线程池请求响应队列</p><h3 id="幂等生产者和事务生产者是一回事吗？-kafka如何做到消息不会丢失，也不会被重复发送"><a href="#幂等生产者和事务生产者是一回事吗？-kafka如何做到消息不会丢失，也不会被重复发送" class="headerlink" title="幂等生产者和事务生产者是一回事吗？(kafka如何做到消息不会丢失，也不会被重复发送)"></a>幂等生产者和事务生产者是一回事吗？(kafka如何做到消息不会丢失，也不会被重复发送)</h3><p>kafka如何做到消息不会丢失，也不会被重复发送?</p><p>Kafka提供消息不会丢失，但可能被重复发送的可靠性保障：</p><ul><li>避免重复生产</li></ul><p>1.创建幂等性Producer，当Producer发送了具有相同字段的消息之后，Broker会知道这些消息已经重复，并在后台进行舍弃。原理是就是经典的空间换时间的优化思想，Broker会在后台多保存一些字段，消息上报时会进行字段内容的核对。</p><p>幂等性 Producer的局限性：单分区幂等性、单会话幂等性</p><p>事务型 Producer:事务型 Producer 能够保证将消息原子性地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。(类似于数据库的串行化)</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="生产者压缩算法"><a href="#生产者压缩算法" class="headerlink" title="生产者压缩算法"></a>生产者压缩算法</h3><ul><li>何时压缩：</li></ul><p>生产者端 和 Broker端</p><p>Broker指定的压缩算法与生产者不一样时，Broker端需要先解压再依据自己的算法算法压缩。</p><ul><li>何时解压</li></ul><p>在consumer端获取的消息中有该消息的压缩算法</p><h3 id="无消息丢失配置怎么实现"><a href="#无消息丢失配置怎么实现" class="headerlink" title="无消息丢失配置怎么实现"></a>无消息丢失配置怎么实现</h3><p>Kafka只对 “已提交的消息”的消息做有限度的持久保证。</p><ul><li>可在一个或若干个Broker成功接收并写入日志文件后，会告诉生产者已提交</li></ul><p>生产者程序丢失数据：Producer永远要使用带有回调通知的发送API</p><p>消费者程序丢失数据：维持先消费消息，再更新位移的顺序</p><p>还有一个解决办法是，多线程异步处理消费信息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移</p><p>总结：</p><ul><li>使用peoducer.send(msg,callback)</li><li>设置acks=all</li><li>设置retries为一个较大的值</li><li>设置unclean.leader.election.enable=false</li><li>设置replication.factor=3</li><li>设置min.insynnc.relicas&gt;1</li><li>确保replication.factor&gt;min.insynnc.relicas</li><li>确保消息消费完成再提交</li></ul><h3 id="客户端都有哪些不常见但是很高级的功能"><a href="#客户端都有哪些不常见但是很高级的功能" class="headerlink" title="客户端都有哪些不常见但是很高级的功能"></a>客户端都有哪些不常见但是很高级的功能</h3><p>Kafka 拦截器分为生产者拦截器和消费者拦截器</p><ul><li>kafka拦截器的使用场景</li></ul><p>Kafka 拦截器可以应用于包括客户端监控、端到端系统性能检测、消息审计等多种功能在内的场景</p><h3 id="Java生产者是如何管理TCP连接"><a href="#Java生产者是如何管理TCP连接" class="headerlink" title="Java生产者是如何管理TCP连接"></a>Java生产者是如何管理TCP连接</h3><ul><li>为何采用TCP？<ul><li>从社区的角度看，在开发客户端时能够利用TCP本身提供的一些高级特性：多路复用请求以及同时轮询多个连接的能力</li><li>目前已知的 HTTP 库在很多编程语言中都略显简陋</li></ul></li><li>TCP连接何时创建？<ul><li>TCP 连接是在创建 KafkaProducer 实例时建立的</li><li>一个是在更新元数据后</li><li>在消息发送时</li></ul></li><li>TCP连接何时关闭？<ul><li>用户主动关闭</li><li>Kafka自带关闭(TTL)</li></ul></li></ul><h3 id="Java-消费者如何管理TCP连接"><a href="#Java-消费者如何管理TCP连接" class="headerlink" title="Java 消费者如何管理TCP连接"></a>Java 消费者如何管理TCP连接</h3><ul><li>何时创建TCP连接？<ul><li>TCP连接是在调用KafkaConsumer.poll 方法时被创建的<ul><li>发起 FindCoordinator 请求时</li><li>连接协调者时</li><li>消费数据时</li></ul></li></ul></li><li>创建多少个 TCP 连接？<ul><li>确定协调者和获取集群元数据</li><li>连接协调者，令其执行组成员管理操作</li><li>执行实际的消息获取</li></ul></li><li>何时关闭连接？<ul><li>手动调用 KafkaConsumer.close() 方法，或者是执行 Kill 命令</li><li>Kafka 自动关闭，由消费者端参数 connection.max.idle.ms 控制的</li></ul></li></ul><h3 id="消费者组到底是什么"><a href="#消费者组到底是什么" class="headerlink" title="消费者组到底是什么?"></a>消费者组到底是什么?</h3><p>Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制</p><p>传统消息引擎模型：点对点模型和发布 / 订阅模型</p><p>点对点模型：消息一旦被消费，就会从队列中删除，而且只能被下游的一个consumer消费。</p><p>缺点：伸缩性（scalability）很差，因为下游的多个 Consumer 都要“抢”这个共享消息队列的消息</p><p>订阅模型: 允许消息被多个 Consumer 消费</p><p>缺点：每个订阅者都必须要订阅主题的所有分区。这种全量订阅的方式既不灵活，也会影响消息的真实投递效果</p><p><em><strong>Kafka 仅仅使用 Consumer Group 这一种机制，却同时实现了传统消息引擎系统的两大模型</strong></em></p><p><em><strong>Rebalance</strong></em> 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区</p><p>触发重平衡的条件：</p><p>1.组员数发生变更</p><p>2.订阅主题数发生变更</p><p>3.订阅主题的分区数发生变更</p><h3 id="揭开神秘的“唯一主题”面纱"><a href="#揭开神秘的“唯一主题”面纱" class="headerlink" title="揭开神秘的“唯一主题”面纱"></a>揭开神秘的“唯一主题”面纱</h3><p>kafka自建位移主题保存consumer的消费位移</p><p>位移主题中的Key保存的内容格式：&lt;Group ID，主题名，分区号 &gt;</p><p>当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题</p><p>如果位移主题是 Kafka 自动创建的，那么该主题的分区数是 50，副本数是 3</p><ul><li>位移主题何时提交？<ul><li>自动提交位移</li><li>手动提交位移</li></ul></li></ul><p>Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可删除数据</p><h3 id="消费组重平衡能避免吗"><a href="#消费组重平衡能避免吗" class="headerlink" title="消费组重平衡能避免吗 ?"></a>消费组重平衡能避免吗 ?</h3><p>原理：</p><p>同一个consumer Group下的所有consumer实例在协调者组件的帮助下完成订阅主题分区的分配</p><p>Broker在启动时会开启相应的Coordinator组件，Broker所属的Coordinator组件与其可能不在同一个节点上。</p><p>Kafka 为某个 Consumer Group 确定 Coordinator 所在的 Broker 的算法：</p><ul><li>确定由位移主题的哪个分区来保存该Group数据</li><li>找出该分区Leader副本所在的Broker,该Broker即为对应的Broker</li></ul><p>重平衡的弊端：</p><ul><li>Rebalance影响Consumer端的TPS，因为重平衡期间消费者不可用</li><li>Rebalance很慢，业务将长时间不可用</li></ul><p>在真实的业务场景中，很多Rebalance都是计划外的或者说是不必要的。</p><p>触发重平衡的条件：</p><p>1.组员数发生变更</p><p>2.订阅主题数发生变更</p><p>3.订阅主题的分区数发生变更</p><p>后两个都是运维层面的不可避免，但是组员数目可以避免。</p><p>避免方式:</p><ul><li>避免consumer未能及时发送心跳而导致被剔除</li><li>避免consumer消费时间过长</li></ul><h3 id="Kafka消息位移提交"><a href="#Kafka消息位移提交" class="headerlink" title="Kafka消息位移提交"></a>Kafka消息位移提交</h3><p>Consumer 需要为分配给它的每个分区提交各自的位移数据</p><p>位移提交的语义保障是由你来负责的，Kafka 只会“无脑”地接受你提交的位移</p><p>从用户的角度来说，位移提交分为自动提交和手动提交；从 Consumer 端的角度来说，位移提交分为同步提交和异步提交</p><p>Kafka提供的提交位移的方法：</p><ul><li>自动提交位移:</li></ul><p>可能会出现重复消费</p><ul><li><p>手动提交位移：</p><ul><li><p>同步提交位移，提交过程中，consumer会处于阻塞状态，知道远端的Broker返回提交结果</p></li><li><p>异步提交位移，异步提交过程失败，重试并没有意义 因为消费的位移已经不是最新值</p><p>所以实际实践需要commitAsync() 避免程序阻塞，Consumer 要关闭前，我们调用 commitSync() 方法执行同步阻塞式的位移提交。</p></li></ul></li><li><p>精细化管理位移</p></li></ul><h3 id="CommitFailedException异常怎么处理？"><a href="#CommitFailedException异常怎么处理？" class="headerlink" title="CommitFailedException异常怎么处理？"></a>CommitFailedException异常怎么处理？</h3><p>当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常。</p><p>处理：</p><ul><li>缩短单条消息处理的时间</li><li>增加 Consumer 端允许下游系统消费一批消息的最大时长</li><li>减少下游系统一次性消费的消息总数</li><li>下游系统使用多线程来加速消费</li></ul><h3 id="多线程开发消费者实例"><a href="#多线程开发消费者实例" class="headerlink" title="多线程开发消费者实例"></a>多线程开发消费者实例</h3><ul><li><p>Kafka Java Consumer 设计原理</p><ul><li><p>Kafka Consumer 是双线程的设计，分为用户主线程和心跳线程</p></li><li><p>原因：</p><p>1.老版本consumer的每个实例都为所订阅的主题分区创建对应的消息获取线程，同时也是阻塞式的，Consumer 实例启动后，内部会创建很多阻塞式的消息获取迭代器，但在很多场景下，Consumer 端是有非阻塞需求的，社区为新版本设计了单线程+轮询的机制</p><p>2.单线程的设计能够简化 Consumer 端的设计。Consumer 获取到消息后，处理消息的逻辑是否采用多线程，完全由你决定。</p></li></ul></li><li><p>Kafka 多线程方案:</p><p>Kafka Consumer 类不是线程安全的 (thread-safe)。所有的网络 I/O 处理都是发生在用户主线程中，因此，你在使用过程中必须要确保线程安全。</p><ul><li>消费者程序启动多个线程，每个线程维护专属的 kafka Consumer实例，负责完整的消息获取、消息处理流程。</li><li>消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑</li></ul></li></ul><h3 id="消费者组消费进度监控怎么实现？"><a href="#消费者组消费进度监控怎么实现？" class="headerlink" title="消费者组消费进度监控怎么实现？"></a>消费者组消费进度监控怎么实现？</h3><p>监控Kafka的滞后程度 Lag</p><p>有三种方法监控：</p><ul><li>Kafka自带命令</li><li>Kafka Java Consumer API</li><li>使用 Kafka 自带的 JMX 监控指标</li></ul><h3 id="消费者组重平衡全流程解析"><a href="#消费者组重平衡全流程解析" class="headerlink" title="消费者组重平衡全流程解析"></a>消费者组重平衡全流程解析</h3><p>依赖消费端的心跳线程来通知其他消费者实例，当需要发生重平衡时，Broker会把需要重平衡的信号封装至心跳上报的响应体中。</p><p>重平衡流程：</p><ul><li>新成员加入：<ul><li>新成员分别发送JoinGroup 请求和 SyncGroup 请求把组员信息发送给调解者，由协调者作为节点的分配</li></ul></li><li>组员主动离组：<ul><li>流程基本同新成员加入</li></ul></li><li>组员崩溃离组：<ul><li>靠心跳线程检测组员状态，由协调者发起重平衡</li></ul></li></ul><h3 id="Kafka控制器"><a href="#Kafka控制器" class="headerlink" title="Kafka控制器"></a>Kafka控制器</h3><p>运行时只能有一个Broker作为控制器，第一个在zookeeper中创建controller节点的Broker会被指定为控制器</p><p>控制器职责：</p><ul><li>主题管理(创建、删除、增加分区)</li><li>分区重分配</li><li>Preferred领导者选举</li><li>集群成员管理(新增Broker 、Broker主动关闭、Briker宕机)</li><li>数据服务</li></ul><p>控制器单点故障转移由zookeeper的watch功能保证通知</p><h3 id="关于高水位和Leader-Epoch"><a href="#关于高水位和Leader-Epoch" class="headerlink" title="关于高水位和Leader Epoch"></a>关于高水位和Leader Epoch</h3><p>Kafka用高水位来表示Kafka中的消息位移，位移值小于高水位的表示已提交的数据，高于高水位的数据表示未提交信息，不能被消费者消费。</p><p>高水位的作用：</p><p>1.定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的</p><p>2.帮助Kafka完成副本同步</p><h3 id="管理和监控-skip"><a href="#管理和监控-skip" class="headerlink" title="管理和监控 skip"></a>管理和监控 skip</h3><h3 id="Kafka-Stream与其他流处理平台的差异在哪"><a href="#Kafka-Stream与其他流处理平台的差异在哪" class="headerlink" title="Kafka Stream与其他流处理平台的差异在哪"></a>Kafka Stream与其他流处理平台的差异在哪</h3><ul><li>Kafka Stream最大的特色就是它不是一个平台，至少它不是一个具备完整功能的平台</li><li>从应用部署方面来看，Kafka Stream倾向于将部署交给开发人员来做，而不是自己实现</li><li>Kafka Stream只支持与Kafka的集群的交换</li><li>Kafka Stream依赖Kafka的协调功能提供高容错性和高伸缩性</li></ul><p>Kafka Stream与consumer的区别是Kafka Stream是实时流处理组件，提供了很多算子，可以实现更多复杂的业务</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用的二次编码方式</title>
      <link href="/2022/03/14/netty/chang-yong-de-er-ci-bian-jie-ma-fang-shi/"/>
      <url>/2022/03/14/netty/chang-yong-de-er-ci-bian-jie-ma-fang-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么需要二次解码"><a href="#为什么需要二次解码" class="headerlink" title="为什么需要二次解码"></a>为什么需要二次解码</h3><p>因为一次解码的结果是字节，需要和项目中所使用的对象做转换，方便使用，这层解码器可以称为“二次解码器”。相应的，对应的编码器是为了将java 对象转化成字节流方便传输存储。</p><p>一次编码器：ByteToMessageDecoder</p><ul><li>io.netty.buffer.ByteBuf(原始数据流) -&gt; io.netty.buffer.ByteBuf（用户数据）</li></ul><p>二次解码器：MessageToMessageDecoder</p><ul><li>io.betty.buffer.ByteBuf(用户数据) -&gt;Java Object</li></ul><h3 id="常用的二次编解码方式"><a href="#常用的二次编解码方式" class="headerlink" title="常用的二次编解码方式"></a>常用的二次编解码方式</h3><ul><li>Java序列化</li><li>Marshing</li><li>XML</li><li>JSON</li><li>MEssagePAck</li><li>Protobuf</li><li>其他</li></ul><h3 id="选择编解码方式的要点"><a href="#选择编解码方式的要点" class="headerlink" title="选择编解码方式的要点"></a>选择编解码方式的要点</h3><ul><li>空间：编码后占用空间</li></ul><h3 id="Protobuf简介与使用"><a href="#Protobuf简介与使用" class="headerlink" title="Protobuf简介与使用"></a>Protobuf简介与使用</h3><h3 id="源码解读：Netty对二次编码的支持"><a href="#源码解读：Netty对二次编码的支持" class="headerlink" title="源码解读：Netty对二次编码的支持"></a>源码解读：Netty对二次编码的支持</h3>]]></content>
      
      
      <categories>
          
          <category> Netty </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编写网络应用程序基本步骤</title>
      <link href="/2022/03/14/netty/bian-xie-wang-luo-ying-yong-cheng-xu-ji-ben-bu-zou/"/>
      <url>/2022/03/14/netty/bian-xie-wang-luo-ying-yong-cheng-xu-ji-ben-bu-zou/</url>
      
        <content type="html"><![CDATA[<p>编写网络应用程序基本步骤：</p><ul><li><p>需求分析</p></li><li><p>定义业务数据结构</p></li><li><p>实现业务逻辑</p></li><li><p>选择传输协议</p></li><li><p>定义传输信息结构</p></li><li><p>选择编解码<br>包括：<br>1、数据本身编解码<br>2、压缩等编解码<br>3、粘包/半包处理编解码</p></li><li><p>实现所有的编解码</p></li><li><p>编写应用程序</p></li><li><p>测试与改进</p></li></ul><p>编写代码-&gt;<br>复查代码-&gt;</p><ul><li>检索”最佳实践” -&gt;检索”坑”-&gt;对比经典项目实现-&gt;同行评审</li></ul><p>临门一脚-&gt;</p><ul><li>检查是否可诊断</li><li>检查是否可度量<br>上线-&gt;反馈-&gt;</li><li>收集错误数据</li><li>收集性能数据</li></ul><p>数据结构设计：</p><p>Frame<br>Message<br>Message Header<br>Message Body<br>length<br>version<br>opCode<br>streamId<br>operation/operation result</p><p>粘包/半包 -&gt; 封帧 -&gt; 加上length字段</p>]]></content>
      
      
      <categories>
          
          <category> Netty </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>netty如何玩转内存使用</title>
      <link href="/2022/03/14/netty/netty-ru-he-wan-zhuan-nei-cun-shi-yong/"/>
      <url>/2022/03/14/netty/netty-ru-he-wan-zhuan-nei-cun-shi-yong/</url>
      
        <content type="html"><![CDATA[<h3 id="Netty如何玩转内存使用"><a href="#Netty如何玩转内存使用" class="headerlink" title="Netty如何玩转内存使用"></a>Netty如何玩转内存使用</h3><p>1、减少对象本身大小</p><p>2、对分配内存进行预估</p><p>3、Zero-Copy 零复制</p><p>4、堆外内存</p><p>优点：<br>1、破除对空间限制，减轻GC压力</p><p>2、避免复制</p><p>缺点：</p><p>1、创建速度稍慢</p><p>2、堆外内存受操作系统管理</p><p>5、内存池</p><p>为什么引入对象池:</p><p>1、创建对象开销大<br>2、对象高频率创建且复用<br>3、支持并发又能保护系统<br>4、维持、共享有限的资源</p><p>如何实现对象池？</p><p>1、开源实现：Apache Commons Pool<br>2、Netty轻量级对象池实现io.netty.util.Recycler</p><h3 id="源码解读Netty内存使用"><a href="#源码解读Netty内存使用" class="headerlink" title="源码解读Netty内存使用"></a>源码解读Netty内存使用</h3><p>1、内存池/非内存池的默认选择及切换方式<br>io.netty.channel.DefaultChannelConfig#allocator</p><p>2、内存池的实现<br>io.netty.buffer.PooledDireByteBuf<br>3、堆外内存/堆内内存的默认选择及切换方式<br>4、堆外内存的分配本质</p>]]></content>
      
      
      <categories>
          
          <category> Netty </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TCP粘包、半包 Netty全搞定</title>
      <link href="/2022/03/13/netty/tcp-nian-bao-ban-bao-netty-quan-gao-ding/"/>
      <url>/2022/03/13/netty/tcp-nian-bao-ban-bao-netty-quan-gao-ding/</url>
      
        <content type="html"><![CDATA[<p>1、什么是粘包和半包？</p><p>粘包：一次接收全部消息</p><p>半包：分多次接收到多个不完整的消息</p><p>2、为什么TCP应用会出现粘包和半包现象</p><p>粘包的主要原因：</p><ul><li>发送方每次写入数据 &lt; 套接字缓冲区大小</li><li>接收方读取套接字缓冲区数据不及时</li></ul><p>半包的主要原因：</p><ul><li>发送方写入数据 &gt; 套接字缓冲区大小</li><li>发送的数据大于协议的MTU(Maximum Transmission Unit，最大传输单元)，必须拆包</li></ul><p>收发：<br>一个发送可能被多次接收，多个发送可能被一次接收</p><p>传输：<br>一个发送可能占用多个传输包，多个发送可能公用一个传输包</p><p>根本原因：TCP是流式协议，消息无边界</p><p>3、解决粘包和半包问题的几种常用方法</p><p>解决问题的根本手段：找出消息边界：</p><table><thead><tr><th align="left">方式\比较</th><th align="left">寻找消息边界方式</th><th align="left">优点</th><th align="left">缺点</th><th align="left">推荐度</th></tr></thead><tbody><tr><td align="left">TCP连接改成短链接，一个请求一个短连接</td><td align="left">建立连接到释放连接之间的信息即为传输消息</td><td align="left">简单</td><td align="left">效率低下</td><td align="left">推荐度</td></tr><tr><td align="left">(封装成帧)固定长度</td><td align="left">满足固定长度即可</td><td align="left">简单</td><td align="left">空间浪费</td><td align="left">不推荐</td></tr><tr><td align="left">（封装成帧）分隔符</td><td align="left">分隔符之间</td><td align="left">空间不浪费，也比较简单</td><td align="left">内容本身出现分隔符时需转义，所以需要扫描内容</td><td align="left">推荐</td></tr><tr><td align="left">(封装成帧)固定长度字段存内容的长度信息</td><td align="left">先解析固定长度的字段获取长度，然后读取后续内容</td><td align="left">精确定位用户数据，内容也不用转义</td><td align="left">长度理论上有限制，需提前预知可能的最大长度从而定义长度占用字节数</td><td align="left">推荐</td></tr><tr><td align="left">(封装成帧)其他方式</td><td align="left">json</td><td align="left">需要衡量实际场景，对现有协议的支持</td><td align="left"></td><td align="left"></td></tr></tbody></table><p>4、Netty对三种常用封帧方式的支持</p><table><thead><tr><th align="left">方式\支持</th><th align="left">解码</th><th align="left">编码</th></tr></thead><tbody><tr><td align="left">固定长度</td><td align="left">FixedLengthFrameDecoder</td><td align="left">简单</td></tr><tr><td align="left">分隔符</td><td align="left">DelimiterBasedFrameDecoder</td><td align="left">简单</td></tr><tr><td align="left">固定长度字段存个内容的长度信息</td><td align="left">LengthFieldBasedFrameDecoder</td><td align="left">LengthFieldPrepender</td></tr></tbody></table><p>5、解读Netty处理粘包、半包的源码</p><ul><li><p>解码的核心工作流程</p></li><li><p>解码中两种数据积累器的区别</p></li><li><p>三种解码器的额外控制参数有哪些</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Netty </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>netty三种IO的支持</title>
      <link href="/2022/03/13/netty/netty-san-chong-io-de-zhi-chi/"/>
      <url>/2022/03/13/netty/netty-san-chong-io-de-zhi-chi/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是经典的三种I-x2F-O模式？"><a href="#什么是经典的三种I-x2F-O模式？" class="headerlink" title="什么是经典的三种I/O模式？"></a>什么是经典的三种I/O模式？</h3><table><thead><tr><th align="left">场景</th><th align="left">模式</th><th align="left">jdk支持</th></tr></thead><tbody><tr><td align="left">排队打饭</td><td align="left">BIO(阻塞I/O)</td><td align="left">jdk1.4之前</td></tr><tr><td align="left">点单、等待被叫模式</td><td align="left">NIO(非阻塞I/O)</td><td align="left">JDK1.4</td></tr><tr><td align="left">包厢</td><td align="left">AIO（非阻塞异步I/O）</td><td align="left">JDK1.7</td></tr></tbody></table><h3 id="Netty对三种I-x2F-O模式的支持？"><a href="#Netty对三种I-x2F-O模式的支持？" class="headerlink" title="Netty对三种I/O模式的支持？"></a>Netty对三种I/O模式的支持？</h3><p>曾经对于三种IO都曾做过支持</p><h3 id="为什么Netty仅支持NIO了？"><a href="#为什么Netty仅支持NIO了？" class="headerlink" title="为什么Netty仅支持NIO了？"></a>为什么Netty仅支持NIO了？</h3><p>1、不建议使用阻塞I/O（BIO/OIO），因为在连接数高的情况下，阻塞意味着占用一个线程，比较耗资源，效率非常低。<br>2、对于不同平台支持的成熟度不同，windows实现成熟，但是很少用来做服务器。linux常用来做服务器，但是AIO不够成熟，并且Linux下AIO相比较NIO的性能提升不明显。</p><h3 id="为什么Netty有多种NIO实现-？"><a href="#为什么Netty有多种NIO实现-？" class="headerlink" title="为什么Netty有多种NIO实现 ？"></a>为什么Netty有多种NIO实现 ？</h3><p>通用的NIO实现在Linux下也是使用epoll，为什么Netty还要单独实现？</p><p>1、因为Netty实现得更好，Netty暴露了更多的可控参数，例如：</p><ul><li>JDK的NIO默认实现是水平触发</li><li>Netty是边缘触发和水平触发可切换</li></ul><p><code>ps：单独解释边缘触发啊和水平触发</code></p><p>2、Netty实现的垃圾回收更少、性能更好</p><h3 id="NIO一定优于BIO吗？"><a href="#NIO一定优于BIO吗？" class="headerlink" title="NIO一定优于BIO吗？"></a>NIO一定优于BIO吗？</h3><p>1、BIO代码简单<br>2、特定场景：连接数少、并发度低、BIO性能不输NIO</p><h3 id="源码解读Netty怎么切换I-x2F-O模式？"><a href="#源码解读Netty怎么切换I-x2F-O模式？" class="headerlink" title="源码解读Netty怎么切换I/O模式？"></a>源码解读Netty怎么切换I/O模式？</h3><p>怎么切换？</p><p>原理是什么？</p><p>为什么服务器开发并不需要切换客户端对应的socket？</p>]]></content>
      
      
      <categories>
          
          <category> Netty </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>netty如何支持三种Reactor</title>
      <link href="/2022/03/13/netty/netty-ru-he-zhi-chi-san-chong-reactor/"/>
      <url>/2022/03/13/netty/netty-ru-he-zhi-chi-san-chong-reactor/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是Reactor及三种版本"><a href="#什么是Reactor及三种版本" class="headerlink" title="什么是Reactor及三种版本"></a>什么是Reactor及三种版本</h3><ul><li><p>Reactor单线程<br>客户端请求到达服务端，服务端创建一个线程处理读取、解码、业务处理、编码、返回响应一整个流程</p></li><li><p>Reactor多线程模式<br>将解码、业务处理、编码的这三个操作都由工作线程池来处理</p></li><li><p>主从Reactor多线程模式</p></li></ul><p>主Reactor负责接收请求，从Reactor负责读请求和写响应，工作线程负责解码、业务处理、编码。</p><p>Reactor是一种开发模式，模式的核心流程：<br>注册感兴趣的事件 -&gt;扫描是否有感兴趣的事件发生 -&gt;事件发生后做出相应处理</p><table><thead><tr><th align="left">client/server</th><th align="left">SocketChannel/ServerSocketChannel</th><th align="left">OP_ACCEPT</th><th align="left">OP_CONNECT</th><th align="left">OP_WRITE</th><th align="left">OP_READ</th></tr></thead><tbody><tr><td align="left">client</td><td align="left">SocketChannel</td><td align="left"></td><td align="left">Y</td><td align="left">Y</td><td align="left">Y</td></tr><tr><td align="left">server</td><td align="left">ServerSocketChannel</td><td align="left">Y</td><td align="left"></td><td align="left"></td><td align="left"></td></tr><tr><td align="left">server</td><td align="left">SocketChannel</td><td align="left"></td><td align="left"></td><td align="left">Y</td><td align="left">Y</td></tr></tbody></table><h3 id="如何在Netty中使用Reactor模式"><a href="#如何在Netty中使用Reactor模式" class="headerlink" title="如何在Netty中使用Reactor模式"></a>如何在Netty中使用Reactor模式</h3><ul><li><p>Reactor单线程模式：</p><pre class="line-numbers language-none"><code class="language-none">EventLoopGroup eventGroup = new NioEventLoopGroup(1);ServerBootstrap serverBootstrap = new ServerBootstrap();serverBootstrap.group(eventGroup);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>非主从Reactor多线程模式</p><pre class="line-numbers language-none"><code class="language-none">EventLoopGroup eventGroup = new NioEventLoopGroup();ServerBootstrap serverBootstrap = new ServerBootstrap();serverBootstrap.group(eventGroup);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>主从Reactor多线程模式</p><pre class="line-numbers language-none"><code class="language-none">EventLoopGroup bossGroup = new NioEventLoopGroup();EventLoopGroup workerGroup = new NioEventLoopGroup();ServerBootstrap serverBootstrap = new ServerBootstrap();serverBootstrap.group(bossGroup, workerGroup)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="解析Netty对Reactor模式支持的常见疑问"><a href="#解析Netty对Reactor模式支持的常见疑问" class="headerlink" title="解析Netty对Reactor模式支持的常见疑问"></a>解析Netty对Reactor模式支持的常见疑问</h3><p>Netty如何支持主从Reactor模式？</p><p>通过传递过来的channel创建子Channel，两种SocketChannel绑定到两个Gruop里面去，这样就完成了主从Reactor模式的支持。</p><p>为什么说Netty的main reactor大多数并不能用到一个线程组，只能线程组里的一个？<br>因为一个服务器一般来说只用绑定一个地址，一个端口</p><p>Netty给Channel分配NIO event loop的规则是什么？</p><p>1、增值、取模、取正值</p><p>2、executors总数是2的幂次方然后&amp;运算</p><p>通常模式的NIO实现多路复用器是怎么跨平台的？<br>通过JDK 读取平台信息 ，创建适合不同平台的实现</p>]]></content>
      
      
      <categories>
          
          <category> Netty </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netty </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
